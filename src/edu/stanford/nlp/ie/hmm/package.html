<body>
Implementation of hidden Markov models for information extraction.

<h2>HMM Information Extraction</h2>
<p>HMMs are potentially more flexible and adaptive models for information 
extraction than regular expressions, and they can be learned from annotated data 
(whereas regular expressions must be hand-specified). However this flexibility 
comes at a cost--HMMs are considerably more complex to specify and training is 
often quite slow. Below we explain the basic framework used in this package and 
highlight some important options for HMM builders.</p>
<p><b>Implementation of <code>FieldExtractor</code> interface: <code>
HMMFieldExtractor</code></b></p>
<ul>
  <li>Build by wrapping a trained HMM</li>
  <li>Input: text</li>
  <li>Output: Map from labels to extracted substrings</li>
  <li>Example: extract company names from article</li>
  <li>Performs extraction using trained hidden Markov model</li>
  <li>Can return best (most likely) answer or first match in document</li>
</ul>
<p><b>Training data: </b>text with XML-tagged target fields</p>
<pre><code>Our speaker today is &lt;speaker&gt;Joseph Smarr&lt;/speaker&gt;.
ENDOFDOC
...</code></pre>
<h3>Data Representation</h3>
<p><b>Internal document representation: <code>TypedTaggedDocument</code></b></p>
<ul>
  <li>Document consisting of <code>TypedTaggedWords</code> (we ignore tags)</li>
</ul>
<table border="1" cellpadding="2" cellspacing="0" style="border-collapse: collapse" bordercolor="#111111" id="AutoNumber1" align="left">
  <tr>
    <td width="12%" align="center"><b><code>Word:</code></b></td>
    <td width="12%" align="center"><code>Our</code></td>
    <td width="12%" align="center"><code>speaker</code></td>
    <td width="12%" align="center"><code>today</code></td>
    <td width="13%" align="center"><code>is</code></td>
    <td width="13%" align="center"><code>Joseph</code></td>
    <td width="13%" align="center"><code>Smarr</code></td>
    <td width="13%" align="center"><code>.</code></td>
  </tr>
  <tr>
    <td width="12%" align="center"><b><code>Type:</code></b></td>
    <td width="12%" align="center"><code>0</code></td>
    <td width="12%" align="center"><code>0</code></td>
    <td width="12%" align="center"><code>0</code></td>
    <td width="13%" align="center"><code>0</code></td>
    <td width="13%" align="center"><code>1</code></td>
    <td width="13%" align="center"><code>1</code></td>
    <td width="13%" align="center"><code>0</code></td>
  </tr>
</table>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><b>Collection of documents: <code>Corpus</code></b></p>
<ul>
  <li>Collection of TypedTaggedDocuments with list of relevant target fields</li>
  <li>Methods for isolating target fields and/or context around target fields 
  (useful for independently training target and context models)</li>
  <li>Reads documents from files using <code>FreitagIECollectionReader</code></li>
</ul>
<p><b>Core implementation: <code>HMM</code></b></p>
<ul>
  <li>Fairly general HMM code; geared for observing words (String emissions)</li>
  <li>Each state emits a word (stochastically) and a target type (fixed per 
  state)</li>
  <li>Can return all extracted fields or just <code>bestAnswers</code> / <code>
  firstAnswers</code></li>
  <li>Calculates <code>logLikelihood</code> and <code>logConditionalLikelihood</code> 
  of documents</li>
  <li>Computes <code>viterbiSequence</code> (highest probability state 
  trajectory)&nbsp; for documents</li>
  <li>Training code (forward-backward, expectations, maximize) inside inner 
  class <code>HMMTrainer</code></li>
</ul>
<h3><b>Building HMMs</b></h3>
<p><b>HMMs are represented internally as an array of <code>State</code>s. Each 
State has:</b></p>
<ul>
  <li><code>double</code> array of transition probabilities to other States</li>
  <li><code>EmitMap</code> specifying emission probabilities for Strings (words)</li>
  <li><code>int</code> label/type (background, company name, etc.)</li>
</ul>
<p><b>Ways to specify HMM transition structure (topology):</b></p>
<ul>
  <li>Hand specify state transitions for array of States</li>
  <li>Use <code>Structure</code> to build up topology<ul>
    <li><code>addTransition</code>, <code>splitState</code>, <code>
    lengthenPrefix</code>, etc.</li>
    <li>Many predefined structures (<code>chrisDefaultStates</code>, <code>
    fmrcComplexStructure</code>, etc.)</li>
  </ul>
  </li>
  <li>Grow structures from simple to complex (<code>StructureLearner</code>)</li>
  <li>Build separate target and context structure and merge them (<code>MergeTrainer</code>)</li>
</ul>
<p><b>Ways to model emissions for each State (EmitMaps):</b></p>
<ul>
  <li><code>PlainEmitMap</code>: raw observed word frequencies (can have 
  sparsity problems)</li>
  <li><code>UnseenEmitMap</code>: interpolates <code>PlainEmitMap</code> with
  <code>FeatureMap</code> (featural decomposition)</li>
  <li><code>ShrinkedEmitMap</code>: interpolates between several EmitMaps</li>
  <li><code>CharSequenceEmitMap</code>: Uses character n-grams (still under 
  construction)</li>
</ul>
<p>Dealing with unknown words is a common problem because 0-probability 
emissions short-circuit the inference process.&nbsp; Using fancy EmitMaps is one 
way to combat this problem.&nbsp; Another is to map rare words to special $UNK$ 
tokens (either a single UNK or different ones for different featural 
decompositions).&nbsp; A further consideration is whether to learn the 
interpolation parameters above on the singletons of training data or on separate 
held out data (see Extractor properties below).</p>
<h3>Training HMMs</h3>
<p>You can construct and train an HMM directly as follows:<br>
<code><br>
State[] states = Structure.chrisDefaultStates();<br>
HMM hmm = new HMM(states, HMM.REGULAR_HMM);<br>
hmm.train(trainingCorpus);<br>
HMMFieldExtractor extractor = new HMMFieldExtractor(hmm, &quot;hmm-chris&quot;);</code></p>
<p>Alternatively you can specify a set of <code>Properties</code> for how the 
HMM should be built and trained.&nbsp; You can use these Properties with <code>
HMMFieldExtractorCreator</code> to get an HMM, or with <code>Extractor</code> 
for standalone training and performance evaluation:</p>
<p><code>HMMFieldExtractorCreator creator = new HMMFieldExtractorCreator();<br>
creator.setProperty(&quot;hmmType&quot;, &quot;chris&quot;);<br>
HMMFieldExtractor extractor = creator.createFieldExtractor(&quot;hmm-chris&quot;);</code></p>
<p>An advantage to using properties is that you can specify design features 
without having to worry about how they're implemented.&nbsp; Properties can also 
be easily stored in files and edited by hand, so they're a convenient way to 
experiment with different settings and save the ones you like. </p>
<p>Extractor is run from the command line to train an HMM with a given set of 
properties. It can optionally test and/or serialize the trained HMM (depending 
on what properties you use).</p>
<p><code>java edu.stanford.nlp.ie.hmm.Extractor [-v] propertyfile</code></p>
<p>The <code>-v</code> option turns on verbose printing, which might not mean 
much to you unless you're familiar with the innards of the HMM implementation.</p>
<h3>HMM Properties</h3>
<p>The class comment for <code>Extractor</code> contains detailed information on 
all available HMM properties, their behavior, and their default values.&nbsp; 
Don't be scared--you can ignore most properties and they will have sensible 
defaults.&nbsp; The top of the Extractor comments walk you through the essential 
properties.</p>
<h3>Visualizing HMMs</h3>
<p>There are various ways to visualize the structure and emissions of a trained 
HMM:</p>
<ul>
  <li>Call <code>printProbs</code> on an HMM to print its transitions and 
  emissions to stderr</li>
  <li>Use <code>HMMViewer</code> to print probs for a serialized HMM</li>
  <li>Use <code>HMMGrapher</code> to display a GUI graph representation of an 
  HMM<ul>
    <li>Opacity of edges between states is proportional to transition weight</li>
    <li>Emission probabilities shown in tooltips for each State</li>
  </ul>
  </li>
</ul>
<h3>References</h3>
<p>This work is based largely on work done by Freitag and McCallum. For more 
descriptions of ideas used, see: </p>
<ul>
  <li>Freitag and McCallum &quot;Information Extraction with HMMs and Shrinkage&quot;. 
  1999. </li>
  <li>Freitag and McCallum. &quot;Information Extraction with HMM Structures Learned 
  by Stochastic Optimization&quot;. AAAI 2000. </li>
  <li>Borkar, Deshmukh, Sarawagi. &quot;Automatic segmentation of text into 
  structured records&quot;. SIGMOD 2001. </li>
</ul>
</body>