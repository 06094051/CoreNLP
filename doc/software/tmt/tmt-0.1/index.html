<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <!-- needed for IE rendering -->
  <!-- <meta http-equiv="X-UA-Compatible" content="IE=edge" /> -->
  
  <title>Stanford Topic Modeling Toolbox</title>
  
  <!-- for use with http://alexgorbatchev.com/wiki/SyntaxHighlighter -->
  <script type="text/javascript" src="js/SyntaxHighlighter/scripts/shCore.js"></script>
  <script type="text/javascript" src="js/SyntaxHighlighter/scripts/shBrushScala.js"></script>
  <link type="text/css" rel="stylesheet" href="js/SyntaxHighlighter/styles/shCore.css"/>
  <link type="text/css" rel="stylesheet" href="js/SyntaxHighlighter/styles/shThemeDefault.css"/>
  <script type="text/javascript">
    SyntaxHighlighter.config.clipboardSwf = 'js/SyntaxHighlighter/scripts/clipboard.swf';
    SyntaxHighlighter.all();
  </script>
  
  <!-- jquery -->
  <script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.3/jquery.min.js"></script>

  <!-- fancybox thumbnails -->
  <link rel="stylesheet" type="text/css" href="js/jquery.fancybox/jquery.fancybox.css" media="screen" />
  <script type="text/javascript" src="js/jquery.fancybox/jquery.easing.1.3.js"></script>
  <script type="text/javascript" src="js/jquery.fancybox/jquery.fancybox-1.2.1.pack.js"></script>
  

  <!-- custom jquery nav code -->
  <script type="text/javascript">
    $(document).ready(function(){
      $("div.sectionbody").hide();
      
      $("a.doShow").click(function(event){
        var base = event.target.id.toString().substring(0,event.target.id.toString().length-".show".length);
        $("div[id="+base+".content]").slideDown();
        $("a.doHide[id="+base+".hide]").show();
        $(this).hide("slow");
        event.preventDefault();
      });
      
      $("a.doHide").click(function(event){
        var base = event.target.id.toString().substring(0,event.target.id.toString().length-".hide".length);
        $("div[id="+base+".content]").slideUp();
        $("a.doShow[id="+base+".show]").show();
        $(this).hide("slow");
        event.preventDefault();
      });
      
      $("a.doPrint").click(function(event){
        $("*").addClass("print");
        $("div.sectionbody").slideDown();
        $("a.doShow").hide();
        $("a.doHide").hide();
        $(this).hide("slow");
        $("a.doScreen").show();
        event.preventDefault();
      });
      
      $("a.doScreen").click(function(event){
        $("*").removeClass("print");
        $("div.sectionbody").slideUp();
        $("a.doShow").show();
        $("a.doHide").hide();
        $(this).hide("slow");
        $("a.doPrint").show();
        event.preventDefault();
      });
      
      $("a.screenshot").fancybox({
        'overlayShow': true
      });
    });
  </script>
   
  <style type="text/css">
    a.doShow { color: #990000; }
    a.doShow:hover { text-decoration: underline; cursor: pointer; }
    a.doHide { color: #990000; display: none; }
    a.doHide:hover { text-decoration: underline; cursor: pointer; }
    
    a.doPrint { color: #990000; float: right; margin-right: 16px; }
    a.doPrint:hover { text-decoration: underline; cursor: pointer; }
    a.doScreen { color: #990000; float: right; margin-right: 16px; display: none; }
    a.doScreen:hover { text-decoration: underline; cursor: pointer; }
    
    div.section {
      width: 100%;
      padding-bottom: 20px;
      margin-top: 20px;
      overflow: hidden;
      clear: left;
    }
    
    div.section h2 {
      border-bottom: 1px solid #990000;
      padding-bottom: .2em;
    }
    

    body {
      margin: 0px 0px 0px 0px;
      padding: 0px 0px 0px 0px;
    }
    
    body.print {
      background-image: none;
    }
  
    div#stripe {
      clear: none;
      position: fixed;
      top: 0px; left: 0px;
      background-image: url('http://nlp.stanford.edu/img/backstripe.gif');
      background-repeat: repeat-y;
      height: 100%;
      width: 21px;
    }
    
    div#stripe.print {
      display: none;
      width: 0px;
      height: 0px;
      clear: none;
    }
  
    div#navbar {
      border: 1px solid #F5EEC6;
      margin-left: 21px;
      background-repeat: no-repeat;
      background-color: #F5EEC6;
    }
    
    div#navbar p {
      font-family: Palatino, Georgia, Book Antiqua, serif;
      font-size: 21px;
      line-height: 24px;
    }
    
    div#navbar a {
      font-size: 12px;
      font-weight: bold;
    }
    
    div#navbar a:link {
      text-decoration: none;
      color: #990000;
      background: #F5EEC6;
    }
    
    div#navbar a:visited {
      text-decoration: none;
      color: #990000;
      background: #F5EEC6;
    }
    
    div#navbar a:hover {
      text-decoration: underline;
    }
    
    div#navbar a:active {
      text-decoration: none;
    }
    
    div#navbar.print {
      visibility:hidden;
      width: 0px;
      height: 0px;
      clear: none;
    }

    div#page {
      margin-left: 35px;
      max-width: 800px;
      text-align: left;
      line-height: 110%;
    }
    
    div#page.print {
      position: static;
      margin-left: 5px;
      margin-top: 0px;
      max-width: 100%;
      width: auto;
      height: auto;
    }
    
    a.screenshot img {
      border: 1px solid gray;
    }
    
    * {
      font-family: Lucida Sans, Arial, Helvetica, sans-serif;
      font-size: small;
      line-height: 1.2em;
    }
    
    h1,h2,h3 {
      font-size: larger;
    }
    
    a img {
      border: none;
    }
  </style>
</head>

<body>

<!--
<script type="text/javascript">
mkNavbar();
alert("hi");
</script>
-->

<div id="navbar">
  <table><tr>
  <td>
    <a href="http://nlp.stanford.edu/"><img
      src="http://nlp.stanford.edu/img/nlp-logo-navbar.jpg"
      alt="Stanford NLP" /></a>
  </td>
  <td>
    <p>The Stanford Natural Language Processing Group</p>
    <a href="http://nlp.stanford.edu/">home</a>
    &middot;
    <a href="http://nlp.stanford.edu/people.shtml">people</a>
    &middot;
    <a href="http://nlp.stanford.edu/teaching.shtml">teaching</a>
    &middot;
    <a href="http://nlp.stanford.edu/research.shtml">research</a>
    &middot;
    <a href="http://nlp.stanford.edu/publications.shtml">publications</a>
    &middot;
    <a href="http://nlp.stanford.edu/software/index.shtml">software</a>
    &middot;
    <a href="http://nlp.stanford.edu/events.shtml">events</a>
    &middot;
    <a href="http://nlp.stanford.edu/local/index.shtml">local</a> 
  </td>
  </tr></table>
</div>
<div id="stripe"></div>


<div id="page">

<h1>Stanford Topic Modeling Toolbox</h1>

<div>
<a class="doPrint">
  <img src="printer.png" alt="Printer Icon [www.famfamfam.com]" width="12px" height="12px" />
  Print-friendly version
</a>
<a class="doScreen">Screen version</a>

Version 0.1.2
</div>

<p>The Stanford Topic Modeling Toolbox (TMT) brings topic modeling tools to social scientists and others who wish to perform analysis on datasets that have a substantial textual component.  The toolbox features that ability to:</p>

<a class="screenshot" rel="group" style="float:right; margin-left: 4em; margin-right: 16px" href="screenshot-0.1.1-0-title.png" title="TMT provides a simple way to access trends in medium size text collections."><img src="screenshot-0.1.1-0-title.thumb.png" alt="" /></a>

<ul>
  <li>Import and manipulate text from cells in Excel and other spreadsheets.</li>
  <li>Train topic models (LDA and Labeled LDA) to create summaries of the text.</li>
  <li>Select parameters (such as the number of topics) via a data-driven process.</li>
  <li>Generate rich Excel-compatible outputs for tracking word usage across topics, time, and other groupings of data.</li>
</ul>

<p>The Stanford Topic Modeling Toolbox was written at the Stanford NLP group by: <br /> <a href="http://cs.stanford.edu/~dramage/">Daniel Ramage</a> and Evan Rosen, first released in September 2009.</p>

<div class="screenshots">
  <a class="screenshot" rel="group" href="screenshot-data-0.png" title="Prepare your data in Excel.  TMT reads standard CSV and TSV files where the text to analyze lives in spreadsheet columns."><img src="screenshot-data-0.thumb.png" alt="" /></a>
  <a class="screenshot" rel="group" href="screenshot-0.1.1-1-ex0.png" title="Load a script from the file menu and prepare to run.  Script access data in CSV and TSV files."><img src="screenshot-0.1.1-1-ex0.thumb.png" alt="" /></a>
  <a class="screenshot" rel="group" href="screenshot-0.1.1-2-postrun.png" title="After clicking Run, the script executes and its output is written into the script output window."><img src="screenshot-0.1.1-2-postrun.thumb.png" alt="" /></a>
  <a class="screenshot" rel="group" href="screenshot-pivot-5.png" title="Load the script output into Excel to generate pivot chart showing topic trends over time."><img src="screenshot-pivot-5.thumb.png" alt="" /></a>
</div>

<p><b>Programmers:</b> See the <a href="scaladocs/">API Reference</a> and the <a href="tmt-0.1.2-src.zip">source code</a>.</p>

<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Getting started</h2>

<p>This section contains software installation instructions and the overviews the basic mechanics of running the toolbox.</p>

<a class="doShow" id="section.getting-started.show">More ...</a>

<div class="sectionbody" id="section.getting-started.content">

<h3>Prerequisites</h3>

<ul>
  <li>A text editor for creating TMT processing scripts.<br/>
      <i>TMT scripts are written in <a href="http://www.scala-lang.org/">Scala</a>, but no knowledge of Scala is required to get started.</i></li>
  <li>An installation of <a href="http://java.sun.com/">Java 6SE</a> or greater: <a href="http://java.com/en/download/index.jsp">download</a>. <br />
    <i>Windows, Mac, and Linux are supported.</i></li>
</ul>

<h3>Installation</h3>

<ul>
  <li>Download and save the TMT executable to your computer. (Sorry, but it's large!) <br />
      <a href="tmt-0.1.2.jar">http://nlp.stanford.edu/software/tmt/tmt-0.1/tmt-0.1.2.jar</a></li>
  <li>Double-click the file to open toolbox.  You should see a simple GUI.  Or from the command line, you can run: <code>java -jar tmt-0.1.2.jar</code></li>
</ul>

<h3>Example data files and scripts</h3>

<ul>
  <li>Download the example data file: <a href="examples/pubmed-oa-subset.csv">pubmed-oa-subset.csv</a></li>
  <li>Download the first example script: <a href="examples/example-0-test.scala">example-0-test.scala</a></li>
</ul>

These two downloads should go into the same folder so that the script can find the data.

<blockquote>
  <p><b>example-0-test.scala</b></p>
  
  <pre class="brush: scala;">
// TMT Example 0 - Basic data loading

import scalanlp.stage.source._;

val pubmed = CSVFile("pubmed-oa-subset.csv");

println("Success: " + pubmed + " contains " + pubmed.data.size + " records");
  </pre>
</blockquote>

<p>This is a simple script that just loads the records contained in the sample data file pubmed-oa-subset, an subset of the Open Access database of publications in Pubmed.</p>

<h3> Running the toolbox</h3>

<p>Now run the toolbox as before and select "Open script ..." from the file menu.  Navigate to <code>example-0-test.scala</code> and click open, then run.</p>

<p>Alternatively, you can run the script from the command line:</p>

<pre>
  java -jar tmt-0.1.2.jar example-0-test.scala
</pre>

<p>If all goes well you should see the following lines of output:</p>

<pre>
  Success: CSVFile("pubmed-oa-subset.csv") contains 1550 records
</pre>

<p>You're all set to continue with the tutorial. For the rest of the tutorial, invoke the toolbox in the same way as we do above but with a different script name.</p>

<a class="doHide" id="section.getting-started.hide">[close section]</a>
</div>
</div>


<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Preparing a dataset</h2>

<p>The first step in using the Topic Modeling Toolbox on a data file (CSV or TSV, e.g. as exported by Excel) is to tell the toolbox where to find the text in the file.  This section describes how the toolbox converts a column of text from a file into a sequence of words.</p>

<a class="doShow" id="section.preparing-a-dataset.show">More ...</a>

<div class="sectionbody" id="section.preparing-a-dataset.content">

<p>The code for this example is in <a href="examples/example-1-dataset.scala">example-1-dataset.scala</a>.</p>

<p>The process of extracting and preparing text from a CSV file can be thought of as a pipeline, where a raw CSV file goes through a series of stages that ultimately result in something that can be used to train the topic model.  Here is a sample pipeline for the pubmed-oa.csv data file:</p>

<pre class="brush: scala">
// input file to read
val pubmed = CSVFile("pubmed-oa-subset.csv");

// the text field extracted and processed from the file
val text = {
    pubmed ~>                            // read from the pubmed file
    Column(3) ~>                         // select column three, the abstracts
    CaseFolder ~>                        // lowercase everything
    SimpleEnglishTokenizer ~>            // tokenize on spaces characters
    WordsAndNumbersOnlyFilter ~>         // ignore non-words and non-numbers
    TermCounter ~>                       // collect counts (needed below)
    TermMinimumLengthFilter(3) ~>        // take terms with &gt;=3 characters
    TermMinimumDocumentCountFilter(4) ~> // filter terms in &lt;4 docs
    TermDynamicStopListFilter(30) ~>     // filter out 30 most common terms
    DocumentMinimumLengthFilter(5)       // take only docs with &gt;=5 terms
}
</pre>

<p>The input data file (in the <code>pubmed</code> variable) is a pointer to the CSV file you downloaded earlier.  <code>pubmed</code> is passed through a a series of <em>stages</em> that each transform, filter, or otherwise interact with the data.</p>

<h3>Selecting data from a CSV file</h3>

<p>The first step is to select fields from your CSV file that contain the text you would like to use for training the model.</p>

<p>If your text data is only in one column:</p>

<pre class="brush: scala">
  CSVFile("your-csv-file.csv") ~> Column(3)
</pre>

<p>The code above will load the text from column three in the CSV file.</p>

<p>If your text is in more than one column:</p>
<pre class="brush: scala">
  CSVFile("your-csv-file.csv") ~> Columns(2,3) ~> Join(" ")
</pre>

<p>The code above select columns two and three, and then concatenates their contents with a space character used as glue.</p>

<h3>Tokenizing</h3>

<p>The next set of manipulations involves breaking up the text into its component words, a process known as tokenization.  This is accomplished with:</p>

<pre class="brush: scala">
  ... ~> CaseFolder ~> SimpleEnglishTokenizer ~> WordsAndNumbersOnlyFilter ~> ...
</pre>

<p>The <code>CaseFolder</code> is first used to make "The" and "tHE" and "THE" all look like "the" &mdash; i.e. the case folder reduces the number of distinct word types seen by the model by turning all character to lowercase.</p>

<p>Next, the text is tokenized using the <code>SimpleEnglishTokenizer</code>, which removes punctuation from the ends of words and then splits up the input text by whitespace characters (tabs, spaces, carriage returns, etc.).</p>

<p>Words that are entirely punctuation and other non-word non-number characters are removed from the generated lists of tokenized documents by using the WordsAndNumbersOnlyFilter.</p>

<h3>Finding meaningful words</h3>

<p>LDA can be useful for extracting patterns in meaningful word use, but it is not good at determining which words are meaningful. It is often the case that the use of very common words like 'the' do not indicate the type of similarity between documents in which one is interested. Single letters or other small sequences are also rarely useful for understanding content.  To lead LDA towards extracting patterns among meaningful words, we have implemented a collection of standard heuristics:</p>

<pre class="brush: scala">
  ... ~> TermCounter ~>
         TermMinimumLengthFilter(3) ~>
         TermMinimumDocumentCountFilter(4) ~>
         TermDynamicStopListFilter(30) ~> ...
</pre>

<p>The code above removes terms that are shorter than three characters (removing, e.g. words like "is"), words that appear in less than four documents (because very rare words tell us little about the similarity of documents), and 30 most common words in the corpus (because words that are ubiquitous also tell us little about the similarity of documents, they are removed and conventionally denoted "stop words").  These values might need to be updated if you are working with a much larger or much smaller corpus than a few thousand documents.</p>

<p>The <code>TermCounter</code> stage must first computes some statistics needed by the next stages.  These statistics are stored in the metadata associated with each parcel, which enables any downstream stage to access that information.</p>

<h3>Removing Empty Documents</h3>

Some documents in your dataset may be missing or empty (now that some words were filtered in the last step).  We can disregard these documents during training by applying the <code>DocumentMinimumLengthFilter(length)</code> to remove all documents shorter than the specified length.

<h3>Putting it all together</h3>

<p>Run example 1 (<a href="examples/example-1-dataset.scala">example-1-dataset.scala</a>).  This program will first load the data pipeline and then print out information about the loaded text dataset, including a signature of the dataset (the "parcel") and the list of 30 stop words found for this corpus.  <i>[Note that in PubMed, "gene" is filtered out because it is so common!]</i></p>

<a class="doHide" id="section.preparing-a-dataset.hide">[close section]</a>
</div>
</div>


<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Learning a topic model</h2>

<p>Once you've prepared a dataset to learn against, you're all set to train a topic model. This example shows how to train an instance of Latent Dirichlet Allocation using the dataset you prepared above.</p>

<a class="doShow" id="section.learning-lda.show">More ...</a>

<div class="sectionbody" id="section.learning-lda.content">

<p>The code for this example is in <a href="examples/example-2-lda-learn.scala">example-2-lda-learn.scala</a></p>

<h3>Load the data file</h3>

<pre class="brush: scala">
// input file to read
val pubmed = CSVFile("pubmed-oa-subset.csv");

// the text field extracted and processed from the file
val text = {
    pubmed ~>                            // read from the pubmed file
    Column(3) ~>                         // select column three, the abstracts
    CaseFolder ~>                        // lowercase everything
    SimpleEnglishTokenizer ~>            // tokenize on spaces characters
    WordsAndNumbersOnlyFilter ~>         // ignore non-words and non-numbers
    TermCounter ~>                       // collect counts (needed below)
    TermMinimumLengthFilter(3) ~>        // take terms with &gt;=3 characters
    TermMinimumDocumentCountFilter(4) ~> // filter terms in &lt;4 docs
    TermDynamicStopListFilter(30) ~>     // filter out 30 most common terms
    DocumentMinimumLengthFilter(5)       // take only docs with &gt;=5 terms
}
</pre>

This code snippet is the same as in the previous tutorial.  It extracts and prepares
the text from column 3.

<h3>Select parameters for training an LDA model</h3>

<pre class="brush:scala">
// turn the text into a dataset ready to be used with LDA
val dataset = LDADataset(text);
 
// define the model parameters
val numTopics = 30;
val modelParams = LDA.ModelParams(numTopics);
   // this is equivalent to:
   //
   // val modelParams = LDA.ModelParams(numTopics,
   //                                   LDA.TermSmoothing(.01),
   //                                   LDA.TopicSmoothing(50.0 / numTopics));
  
// define the training parameters
val trainingParams = GibbsLDATrainer.DefaultTrainingParams;
    // this is equivalent to:
    //
    // import GibbsLDA.LearningModel._;
    // val trainingParams =
    //   TrainingParams(MaxIterations(1500),
    //                  SaveEvery(50, LogProbabilityEstimate,
    //                                DocumentTopicDistributions,
    //                                DocumentTopicAssignments));
    //
    // SaveEvery(...) could be replaced by SaveFinal() to write less output
    //
    // val trainingParams = TrainingParams(MaxIterations(1500), SaveFinal());
</pre>

<h3>Train the model to fit the documents</h3>

<pre class="brush: scala;">
// Name of the output model folder to generate
val output = file("lda-"+dataset.signature+"-"+modelParams.signature);

// Trains the model: the model (and intermediate models) are written to the
// output folder.  If a partially trained model with the same dataset and
// parameters exists in that folder, training will be resumed.
GibbsLDA.train(output, dataset, modelParams, trainingParams);
</pre>

<p>The model will output status message as it trains. It'll take a few minutes.</p>


<h3>A tour of the generated output folder</h3>

<p>The generated model output folder, in this case <code>lda-f7a35bfa-30-2b517070-7c1f94d2</code>, contains everything needed to analyze the learning process and to load the model back in from disk.</p>

<table>
  <tr><td>dataset.txt</td>
      <td>The history of stages used to get the text used for training.</td></tr>
  <tr><td>model-params.txt</td>
      <td>The model parameters specified during training.</td></tr>
  <tr><td>training-params.txt</td>
      <td>The training parameters used to determine convergence.</td></tr>
  <tr><td>00000 - 01500</td>
      <td>napshots of the model during training every 50 iterations.</td></tr>
</table>

<h3>Determining if the model has converged</h3>

<p>A simple way to see if the training procedure on the model has converged is to look at the values in the numbered folders of <code>log-probability-estimate.txt</code>. This file contains an informal estimate of the model's estimation of the probability of the data while training. The numbers tend to make a curve that tapers off but never stops changing completely. If the numbers don't look like they've stabilized, you might want to retrain using a higher number of iterations.  If you re-run the script with a higher number of iterations (or if continue training a model that was interrupted during traiing), the toolbox will continue training the model from the highest iteration stored on disk.</p>

<a class="doHide" id="section.learning-lda.hide">[close section]</a>

</div>
</div>


<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Selecting model parameters</h2>

<p>This tutorial describes how to select model parameters such as the number of topics by a (computationally intensive) tuning procedure, which searches for the parameters that minimize the model's perplexity on held-out data.</p>

<a class="doShow" id="section.tmt-tuning.show">More ...</a>

<div class="sectionbody" id="section.tmt-tuning.content">

<p>The code for this example is in <a href="examples/example-4-lda-select.scala">example-4-lda-select.scala</a></p>

<p>The script splits a document into two subsets: one used for training models, the other used for evaluating their <em>perplexity</em> on unseen data.  Perplexity is scored on the evaluation documents by first splitting each document in half.  The per-document topic distribution is estimated on the first half of the words.  The toolbox then computes an average of how surprised it was by the words in the second half of the document, where surprise is measured in the number of equiprobable word choices, on average.  The value is written into each trained model's output folder as <code>perplexity.txt</code>, with lower numbers meaning a surer model.</p>

<p>The perplexity scores are not comparable across corpora because they will be affected by different vocabulary size.  However, they can be used to compare models trained on the same data (as in the example script).  However, be aware that models with better perplexity scores don't always produce more interpretable topics or topics better suited to a particular task.  Perplexity scores can be used as stable measures for picking among alternatives, for lack of a better option.</p>

<p>Some non-parametric topic models can automatically select the number of topics as part of the model training procedure itself.  However, these models (such as the Hierarchical Dirichlet Process) are not yet implemented in the toolbox.  Even in such models, some parameters remain to be tuned, such as the topic smoothing and term smoothing parameters.</p>

</div>
</div>

<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Generating topic model outputs</h2>

<p>This tutorial shows how to generate basic outputs by querying the topic model for information about topic and word usage in various subsets of the data. </p>

<a class="doShow" id="section.tmt-outputs.show">More ...</a>

<div class="sectionbody" id="section.tmt-outputs.content">

<p>The code for this example is in <a href="examples/example-3-lda-infer.scala">example-3-lda-infer.scala</a></p>

<h3>Load the text as before</h3>
<pre class="brush: scala">
// input file to read
val pubmed = CSVFile("pubmed-oa-subset.csv");

// the text field extracted and processed from the file
val text = {
    pubmed ~>                            // read from the pubmed file
    Column(3) ~>                         // select column three, the abstracts
    CaseFolder ~>                        // lowercase everything
    SimpleEnglishTokenizer ~>            // tokenize on spaces characters
    WordsAndNumbersOnlyFilter ~>         // ignore non-words and non-numbers
    TermCounter ~>                       // collect counts (needed below)
    TermMinimumLengthFilter(3) ~>        // take terms with &gt;=3 characters
    TermMinimumDocumentCountFilter(4) ~> // filter terms in &lt;4 docs
    TermDynamicStopListFilter(30) ~>     // filter out 30 most common terms
    DocumentMinimumLengthFilter(5)       // take only docs with &gt;=5 terms
}

// turn the text into a dataset ready to be used with LDA
val dataset = LDADataset(text);
</pre>

<h3>Re-load the trained model</h3>

<pre class="brush: scala;">
// the path to the model we want to load
val modelPath = file("lda-f7a35bfa-30-2b517070-7c1f94d2");

// load the trained model
val model = GibbsLDA.loadInferenceModel(modelPath);
</pre>

<h3>Infer per-document per-word distributions over latent topics</h3>

<pre class="brush: scala;">
// infer topic distributions for each word in each document in the dataset.
System.err.println("Running inference ... (this could take several minutes)");
val perDocWordTopicProbability = InferPerWord(model, dataset);
</pre>

<h3>Generating global topic statistics</h3>

<p>Our implementation currently supports three primary queries on the product of inference.</p>

<ul>
  <li>Top-k words per topic</li>
  <li>Usage of individual words in each topic</li>
  <li>Usage of topics overall in the corpus</li>
</ul>

The example snippet saves these outputs into CSV files that can be imported back into Excel. 
<pre class="brush: scala;">
//
// now build an object to query the inferred outputs
//

System.err.println("Generating general outputs ...");

// build an object to query the model
val fullLDAQuery = LDAQuery(perDocWordTopicProbability);

// write the top 20 words per topic to a csv file
fullLDAQuery.topK(20) | CSVFile("pubmed-topk.csv");

// track some words' usage
fullLDAQuery.trackWords("gene","probability") | CSVFile("pubmed-words.csv");

// write the overall topic usage
fullLDAQuery.usage | CSVFile("pubmed-usage.csv");
</pre>

<p>The statistic generated is (fractional) words - i.e. how many words were assigned to the given topic.  The fractions come from the fact that each word really gets a distribution over topic assignments, instead of just a single topic assigned to.  An upcoming revision could include (fractional) documents as another column if there's demand for it.</p>


<h3>Slicing the LDA query output</h3>

<p>Often, more data is associated with each document than just its words. In our example, we also have a year associated with each document. We can query by "slices" of the dataset, which subdivide the counts as generated above by metadata listed in the grouping information of each document.</p>

<pre class="brush: scala;">
//
// now build an object to query by a field
//

System.err.println("Generating sliced outputs ...");

// define fields from the dataset we are going to slice against
val year = pubmed ~> Column(1);   // select column 1, the year
             
// create a slice object by binding the output of inference with the fields
val sliceLDAQuery = SlicedLDAQuery(perDocWordTopicProbability, year);

sliceLDAQuery.topK(20) | CSVFile("pubmed-slice-topk.csv");
sliceLDAQuery.trackWords("gene","probability") | CSVFile("pubmed-slice-trackwords.csv");
sliceLDAQuery.usage | CSVFile("pubmed-slice-usage.csv");
</pre>

<p>The next tutorial shows you what you can do with these outputs in more detail.</p>

<a class="doHide" id="section.tmt-outputs.hide">[close section]</a>

</div>
</div>


<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Analyzing topic model outputs in Excel</h2>

<p>The CSV files generated in the previous tutorial can be directly imported into Excel to provide an advanced analysis and plotting platform for understanding, plotting, and manipulating the topic model outputs. If things don't seem to make sense, you might need to try different model parameters.</p>

<div class="screenshots">
  <a class="screenshot" rel="pivot" href="screenshot-pivot-0.png" title="Load the sliced LDA output into excel, add a header row, and insert a pivot chart."><img src="screenshot-pivot-0.thumb.png" alt="" /></a>
  <a class="screenshot" rel="pivot" href="screenshot-pivot-1.png" title="If you already selected all cells, just press OK."><img src="screenshot-pivot-1.thumb.png" alt="" /></a>
  <a class="screenshot" rel="pivot" href="screenshot-pivot-2.png" title="Now add Year to Axis fields, Topic to Legend fields and Counts to Value fields."><img src="screenshot-pivot-2.thumb.png" alt="" /></a>
  <a class="screenshot" rel="pivot" href="screenshot-pivot-3.png" title="Change the chart type to a Line graph ..."><img src="screenshot-pivot-3.thumb.png" alt="" /></a>
  <a class="screenshot" rel="pivot" href="screenshot-pivot-4.png" title="And select which topics to show by clicking the drop-down filter by Column Labels."><img src="screenshot-pivot-4.thumb.png" alt="" /></a>
  <a class="screenshot" rel="pivot" href="screenshot-pivot-5.png" title="Now you can explore topic trends over time."><img src="screenshot-pivot-5.thumb.png" alt="" /></a>
</div>


<!--
<a class="doShow" id="section.excel-analysis.show">More ...</a>

<div class="sectionbody" id="section.excel-analysis.content">

</div>
-->

</div>

</div>
</body>
</html>
