#### NOTE: for all flags and their description, see the javadoc. Important parameters (in our experience) that you should tune for your dataset are marked with *** 

numThreads=1

#name for the saved files for the output of the system (useful for comparing results of different experiments with different variables etc
identifier=useNERRestriction

#Directory where this code lives
DIR=projects/core/data/edu/stanford/nlp/patterns/surface/

outDir=patternsout

### Example for running it on presidents biographies. For more data examples, see the bottom of this file

#can be text. the code will tokenize it.
fileFormat=text
#Input file(s) (default assumed text). Can be one or more of (concatenated by comma or semi-colon): file, directory, files with regex in the filename (for example: "mydir/health-.*-processed.txt")
file=${DIR}/presidents.txt

#to save the serialized sentences into a file - text split into sentences, processed using ner, parse etc (depending on the flags) and labeled with seed set
#saveSentencesSerFile=${DIR}/presidents_sents.ser

#if you use the flag above to save the file, you can use the saved file like this
#fileFormat=ser
#file= ${DIR}/presidents_sents.ser


#We are learning names of presidential candidates, places, and other names 
seedWordsFiles=NAME,${DIR}/names.txt;PLACE,${DIR}/places.txt;OTHER,${DIR}/otherpeople.txt
evaluate=true
goldEntitiesEvalFiles=NAME,${DIR}/goldnames.txt;PLACE,${DIR}/goldplaces.txt

#posModelPath=<if you want to use a different Stanford NLP group released POS tagger; e.g. caseless etc>

######## creating patterns flags ##########
#***use context on the left
usePreviousContext=true

#***use context on the right
useNextContext = true

#***the context should be at least this long
minWindow4Pattern = 2

#***the context can be at most this long
maxWindow4Pattern = 4

#if the context consists of only stop words, add only if it's more than these many stop words
numMinStopWordsToAdd = 3

#***use POS tag restriction for the target phrase
usePOS4Pattern = false

#Ignore words {a, an, the} while matching the patterns to text (advisable true)
useFillerWordsInPat = false

#***Allowed tag for the target phrase. Tag initials can be written as N or NN or J or N,J etc
allowedTagsInitials = * 

#You can save all possible patterns for all tokens in the flag allPatternsFile so you wouldn't need to calculate them everytime.
computeAllPatterns = true

#Save or read (if computeAllPatterns is false) from here 
allPatternsFile= ${DIR}/${identifier}_allpatterns.ser

#***maximum Num of allowed words in the target phrase
numWordsCompound = 3

#***consider patterns without the POS restricion on the target phrase
addPatWithoutPOS = true

#Ignore common stop words occuring just before the target phrase
useStopWordsBeforeTerm=false

#Use lemma instead of words of the context tokens
useLemmaContextTokens=false

#make context matching lowercase (advisable)
matchLowerCaseContext=true

#***use named entity tag (predicted using StanfordCoreNLP NER) restriction of the target phrase
useTargetNERRestriction=true

#use named entity tag restrictions for the context tokens
useContextNERRestriction=false

#***use the parse tag of the grandparent node as restriction (note that parent node is the POS tag)
useTargetParserParentRestriction=false

#do not extract phrase in which any word is labeled with another class (for example, you don't wanna extract 'HIV patients' as disease)
doNotExtractPhraseAnyWordLabeledOtherClass = true

#### matching patterns to text ######

#kinda ignore this flag and use it as true. for those who care this too much: for each token, we use the phrase that originally matched that token instead of the token's word
useMatchingPhrase=true

#Use only the tokens that get matched by a pattern (advisable as false)
restrictToMatched = false

#Label the learned words in the text (advisable as true)
usePatternResultAsLabel=true

#remove common stop words from phrases to get clean phrases (for example, "disease" instead of "some disease")
removeStopWordsFromSelectedPhrases = true

#Do not learn phrases that have any stop word 
removePhrasesWithStopWords = false


### evaluating candidate patterns

#REMOVE THIS NOT USED***discard patterns that do not have any unlabeled entity associated right now (caution: if the pattern has many wildcards, it might actually extract unlabeled entities from text later!)
#discardPatternsWithNoUnlabSupport=true

#Minimum number of unlabeled phrases a candidate pattern should extract
minUnlabPhraseSupportForPat = 1

#***Minimum number of positive phrases a candidate pattern should extract
minPosPhraseSupportForPat = 1

##### thresholds for selecting paterns and words #####

#***threshold for learning a phrase
thresholdWordExtract=0.2

#***thrshold for learning a pattern
thresholdSelectPattern = 0.001

#keep lowering threshold as 0.8*threshold whenever the system doesn't learn any new patterns and phrases
tuneThresholdKeepRunning=false

#***discard phrases that do not have these many patterns extracting it
thresholdNumPatternsApplied = 1

#***max number of words to extract in each iteration
numWordsToAdd = 2

#***max number of words to extract in each pattern
numPatterns = 5

#***max number of iterations
numIterationsForPatterns = 4

#Consider words belonging to other labels as negative (advisable as true)
useOtherLabelsWordsasNegative=true

#***Pattern scoring measure. For more details, see the paper. The options are PhEvalInPatLogP, PhEvalInPat, PosNegUnlabOdds, RlogF, RlogFPosNeg, YanGarber02, PosNegOdds, LOGREG, RatioAll, SqrtAllRatio
patternScoring=PhEvalInPat

#if you want to sqrt root the pattern score
sqrtPatScore = false

#Phrase scoring measure; ignore.
wordScoring=WEIGHTEDNORM

#For scoring phrases that are OOV, a score is the average of the score of individual words (instead of min, which is default)
useAvgInsteadofMinPhraseScoring=true

#*** what all features to use to evaluate phrases. See the paper for more details on each
#only if wordClassClusterFile is provided
usePhraseEvalWordClass=false

#tf-idf scoring w.r.t to the domain 
usePhraseEvalDomainNgram=false

#use pattern weights in scoring phrases extracted by them, if usePhraseEvalPatWtByFreq is true. otherwise it's just a tfidf like score 
usePatternWeights=true

#basically patwt/log(freq), patwt = 1 if usePatternWeights is false
usePhraseEvalPatWtByFreq=true

#if using multiple label dictionaries etc, freq of the phrase in the label dictionary vs other dictionaries
usePhraseEvalSemanticOdds=true

#edit distance from positive entities
usePhraseEvalEditDistSame=true

#edit distance from the negative entities
usePhraseEvalEditDistOther=true

#if you have googlengrams, you can use googlengrams tf-idf scoring.
usePhraseEvalGoogleNgram=false

#These flags are not valid if patternScoring is not PhEvalInPat* . Similar meaning as for the phrase ones above
usePatternEvalWordClass=false
usePatternEvalGoogleNgram=false
usePatternEvalSemanticOdds=true
usePatternEvalEditDistSame=true
usePatternEvalEditDistOther=true
#false works best for below
usePatternEvalDomainNgram=false

#Options are LOG, NONE or SQRT
wordFreqNorm = NONE

######For logging

#if you wanna print out every single thing happening in the system, if using this, also use learnPatternsDebug as true
extremedebug=false
#if you want fair amount of debug messages
learnPatternsDebug = true
#In the debug msg, justify why a pattern or entity was learned
justify = true

markedOutputTextFile=markedtext.txt


#stop words file
stopWordsPatternFiles=${DIR}/stopwords.txt

englishWordsFiles=${stopWordsPatternFiles} 
commonWordsPatternFiles= ${stopWordsPatternFiles} 
#You can give some common words like this
#commonWordsPatternFiles =${DIR}/lists/commonEngWords1k.txt

#If you are using Google Ngrams TF-IDF feature
#googleNGramsFile=/u/nlp/scr/google-ngrams/1gms/vocab
#weightDomainFreq=10


### Example for running the code on BioMed articles and NCBI corpus (instead of the toy example above)

#fileFormat=text
#file=${DIR}/BioMedSample
#saveSentencesSerFile=${DIR}/biomed_sents.ser

#evalFileWithGoldLabels=${DIR}/NCBI_corpus_testing_processed.txt
#saveEvalSentencesSerFile=${DIR}/ncbi_corpus_testing_sents.ser
#addEvalSentsToTrain=true

#seedWordsFiles=disease,${DIR}/diseases.txt;nondisease,${DIR}/nondiseases.txt

#evaluate=true

#default as true, false if you want scores per token
#evalPerEntity=true

#wordClassClusterFile=${DIR}/ncbi_disease_brownclusters_200_min5.txt

#externalFeatureWeightsFile = ${DIR}/out/wordclass_weights

#perSelectRand=0.5

#perSelectNeg=1

#below is optional; comma separated files with list of phrases that def do not belong to any of the labels
#otherSemanticClassesFiles=${DIR}/nondiseases.txt

#false if you just want to process the text into sents but not do anything with it. Useful for batch processing and saving text as serialized objects, then running the learning system on all the serialized objects (see saveSentences* and saveEvalSent* flags)
learn=true

