
# Pipeline options
annotators = pos, lemma, parse, ner
#pos.model = /u/nlp/data/pos-tagger/wsj3t0-18-bidirectional/bidirectional-distsim-wsj-0-18.tagger
#ner.model.3class = /u/nlp/data/ner/goodClassifiers/all.3class.distsim.crf.ser.gz
#ner.model.7class = /u/nlp/data/ner/goodClassifiers/muc.distsim.crf.ser.gz
#ner.model.MISCclass = /u/nlp/data/ner/goodClassifiers/conll.distsim.crf.ser.gz
#parser.model = /u/nlp/data/lexparser/englishPCFG.ser.gz
parser.maxlen = 100

# MachineReading properties
datasetReaderClass = edu.stanford.nlp.ie.machinereading.domains.nfl.NFLReader
relationMentionFactoryClass = edu.stanford.nlp.ie.machinereading.domains.nfl.NFLRelationMentionFactory
relationExtractionPostProcessorClass = edu.stanford.nlp.ie.machinereading.domains.nfl.BasicNFLInference
logLevel = INFO
readerLogLevel = INFO

# Andrey's corpus generated using Yahoo! Sports and MTurk
#datasetAuxReaderClass = edu.stanford.nlp.ie.machinereading.domains.nfl.AMTResultNFLReader
#auxDataPath = C:/dev/javanlp-data/AMT relations/results/results_50_relations.csv
#serializedAuxTrainingSentencesPath = tmp/nfl_aux_training_sentences.ser

# David's corpus generated using automated annotations and Yahoo! Sports
#auxDataPath = /scr/nlp/data/machine-reading/NFL_MTurk/rel_xml
#datasetAuxReaderClass = edu.stanford.nlp.ie.machinereading.domains.nfl.StringBasedNFLReader
#serializedAuxTrainingSentencesPath = /home/mihais/code/javanlp/tmp/nfl_aux_training_sentences.ser

# Mihai's corpus, which extends David's with automatically-generated relations (note: this corpus has correct offsets)
#auxDataPath = /juicy/scr61/scr/nlp/data/machine-reading/NFL_Crawled_Generated/rel_xml
#datasetAuxReaderClass = edu.stanford.nlp.ie.machinereading.domains.nfl.NFLReader
#serializedAuxTrainingSentencesPath = /home/mihais/code/javanlp/tmp/nfl_auxgen_training_sentences.ser

# training
# original NFL data: /scr/nlp/data/machine-reading/Machine_Reading_P1_Reading_Task_V2.0/data/SportsDomain/NFLScoring_UseCase/syntax_mapping/
# new NFL data in ".gui.xml" format: /scr/nlp/data/machine-reading/Machine_Reading_P1_NFL_Scoring_Training_Data_V1.1/data/SportsDomain/NFL_Scoring/gui_xml
trainPath = /scr/nlp/data/machine-reading/Machine_Reading_P1_NFL_Scoring_Training_Data_V1.2/data/SportsDomain/NFL_Scoring/rel_xml
serializedTrainingSentencesPath = /home/mihais/code/javanlp/tmp/nfl_training_sentences.ser

crossValidate = true
kfold = 10

# set crossValidate to false and uncomment lines below to train a model on all training data (for faust distribution)
#testPath = /scr/nlp/data/machine-reading/Machine_Reading_P1_NFL_Scoring_Training_Data_V1.2/data/SportsDomain/NFL_Scoring/rel_xml
#serializedTestSentencesPath = /home/mihais/code/javanlp/tmp/nfl_testing_sentences.ser

# gazetteer of NFL teams
entityGazetteerPath = /scr/nlp/data/machine-reading/Machine_Reading_P1_Reading_Task_V2.0/data/SportsDomain/NFLScoring_UseCase/NFLgazetteer.txt
#entityGazetteerPath = /scr/nlp/data/machine-reading/Machine_Reading_P1_Reading_Task_V2.0/data/SportsDomain/NFLScoring_UseCase/NFLEmptyGazetteer.txt

# what entity tagger to use
entityClassifier = edu.stanford.nlp.ie.machinereading.domains.nfl.NFLEntityExtractor
# use the one below for the max-recall NER setup (this is much worse in the latest NFL corpus)
# entityClassifier = edu.stanford.nlp.ie.machinereading.domains.nfl.NFLMaxRecallEntityExtractor

# where to store the models
# note: assumes that a tmp/ directory exists relative to the execution path
serializedEntityExtractorPath = /home/mihais/code/javanlp/tmp/nfl_entity_model.ser
serializedRelationExtractorPath = /home/mihais/code/javanlp/tmp/nfl_relation_model.ser

entityResultsPrinters = edu.stanford.nlp.ie.machinereading.domains.nfl.NFLEntityExtractorResultsPrinter
# use the one below for the max-recall NER setup
# entityResultsPrinters = edu.stanford.nlp.ie.machinereading.domains.nfl.NFLMaxRecallEntityExtractorResultsPrinter
relationResultsPrinters = edu.stanford.nlp.ie.machinereading.domains.nfl.NFLRelationExtractorResultsPrinter

# set this to true to test the relation model using predicted entities
testRelationsUsingPredictedEntities = true

# The set chosen by feature selection using RothCONLL04
relationFeatures = arg_words,arg_type,dependency_path_lowlevel,dependency_path_words,surface_path_POS,entities_between_args,full_tree_path
# The above features plus the features used in Bjorne BioNLP09
# relationFeatures = arg_words,arg_type,dependency_path_lowlevel,dependency_path_words,surface_path_POS,entities_between_args,full_tree_path,dependency_path_POS_unigrams,dependency_path_word_n_grams,dependency_path_POS_n_grams,dependency_path_edge_lowlevel_n_grams,dependency_path_edge-node-edge-grams_lowlevel,dependency_path_node-edge-node-grams_lowlevel,dependency_path_directed_bigrams,dependency_path_edge_unigrams,same_head,entity_counts

# set this to true to load a pre-trained serialized model instead of training
loadModel = true

extractEntities = true
extractRelations = true
extractEvents = false

