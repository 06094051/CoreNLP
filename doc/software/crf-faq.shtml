<!--#include virtual="/header.html" -->

<center>
<h2><font color="#a40526"><a href="http://nlp.stanford.edu/software/CRF-NER.shtml">Stanford NER CRF</a> FAQ</font></h2>
</center>

<h3>Questions</h3>

<ol>
<li><a href="#g">How can I unpack the gzipped tar file?</a></li>
<li><a href="#a">How can I train my own NER model?</a></li>
<li><a href="#b">That wasn't enough information. Can you really tell me how can I train my own NER model?</a></li>
<li><a href="#d">How can I train an NER model using less memory?</a></li>
<li><a href="#mfiles">How do I train one model from multiple files?</a></li>
<li><a href="#i">What is the API for using CRFClassifier in a program?</a></li>
<li><a href="#c">Can I set the Stanford NER system up to allow single-jar
deployment rather than it having to load NER models from separate files?</a></li>
<li><a href="#cc">For our Web 5.0 system, can I run Stanford NER as a server/service/servlet?</a></li>
<li><a href="#e">Why do I get compilation errors when I try to compile
recent versions of the source code (such as the 2008-05-07 version)?</a></li>
<li><a href="#h">How can I use the version of <code>NERDemo.java</code>
included in the 2008-05-07 version of the software?</a></li>
<li><a href="#j">What options are available for formatting the output of the classifier?</a></li>
<li><a href="#k">Why can't I reproduce the results of your CoNLL 2003
system?  Or: How do I get better CoNLL 2003 English performance?</a></li>
<li><a href="#l">How do I get German NER working right?  My characters
    are messed up.</a></li>
<li><a href="#mem">What is the asymptotic memory growth of the classifier?</a></li>
<li><a href="#extend">Can an existing model be extended?</a></li>
<li><A href="#data">Can you release the data used to build the publicly released models</a></li>
<li><a href="#deterministic">Is the NER deterministic?  Why do the results change for the same data?</a></li>

<!-- <li><a href="#f">Can you use gazettes (name lists) with Stanford NER?</a></li> -->
</ol>

<h3>Questions with answers</h3>

<ol>


<li><h4><a name="g">How can I unpack the gzipped tar file?</a></h4>
<p>
On Unix/Linux (or command-line Mac OS X), use GNU tar, if you're not already.  (If you're using
Linux, you're almost certainly using GNU tar.)  For some reason we don't
understand, it doesn't seem to unpack with classic Unix tar.  Make sure you
specify the <code>-z</code> option if you are not gunzipping it in
advance: <code>tar -xzf stanford-ner-2009-01-16.tgz</code>.</p>

<p>On Windows, it unpacks fine
with most common tools, such as WinZip or 7-Zip.  The latter is open
source. (As of Sep 2007, WinRAR doesn't work: it apparently does not handle
tar files correctly.)</p>

<p>On the Mac, just double-click it to unpack.  The default unarchiver
(BOMArchiveHelper) works fine.</p>

<p>If it won't unpack, you normally have either a corrupted download
(try downloading it again), a defective tool for unpacking tar files
(normally evidenced by Unix file protection bits like 0664 appearing in
file names), or
there is some configuration error on your system, which we can't help
with.
</p>


<li><h4><a name="a">How can I train my own NER model?</a></h4>
<p>
The documentation for training your own classifier is certainly
somewhere between bad and non-existent.  But nevertheless, everything
you need is in the box, and you should
look through the Javadoc for at least the classes
<code>CRFClassifier</code> and 
<code>NERFeatureFactory</code>.
</p><p>
Basically, the training data should be in tab-separated columns, and you
define the meaning of those columns via a map.  One column should be
called "answer" and has the NER class, and existing features know about
names like "word" and "tag".  You define the data
file, the map, and what features to generate via a properties
file. There is considerable documentation of what features different
properties generate in the Javadoc of NERFeatureFactory, though
ultimately you have to go to the source code to answer some
questions....
</p><p>
Here's a sample NER properties file:
</p>
<blockquote><pre>
trainFile = training-data.col
serializeTo = ner-model.ser.gz
map = word=0,answer=1

useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
useDisjunctive=true
</pre>
</blockquote>
</li>

<li><h4><a name="b">That wasn't enough information. Can you really tell me how can I
train my own NER model?</a></h4>
<p>
Oh, okay.  Here's an example.  Suppose we want to build an NER system for
Jane Austen novels.  We might train it on <a
href="ner-example/jane-austen-emma-ch1.txt">chapter 1 of
<i>Emma</i></a>.  Download that file.  You can convert it to one token
per line with our tokenizer (included in the box) with the following command:
</p>
<blockquote><code>
java -cp stanford-ner.jar  edu.stanford.nlp.process.PTBTokenizer
jane-austen-emma-ch1.txt > jane-austen-emma-ch1.tok
</code></blockquote>
<p>We then need to make training data where we label the entities.
There are various annotation tools available, or you could do this by
hand in a text editor.  One way is to default to making everything an
other (for which the default label is "O" in our software, though you
can specify it via the <code>backgroundSymbol</code> property) and then to
hand-label the real entities in a text editor.  The first step can be
done with Perl using this command:
</p>
<blockquote><code>
perl -ne 'chomp; print "$_\tO\n"' jane-austen-emma-ch1.tok &gt; jane-austen-emma-ch1.tsv
</code></blockquote>
<p>
and if you don't want to do the second, you can skip to downloading <a
href="ner-example/jane-austen-emma-ch1.tsv">our input file</a>.  We have marked only
one entity type, PERS for person name, but you could easily add a second
entity type such as LOC for location, to this data.
</p>
<p>
You will then also want some test data to see how well the system is
doing.  You can download the text of 
<a href="ner-example/jane-austen-emma-ch2.txt">chapter 2 of <i>Emma</i></a>
and 
<a href="ner-example/jane-austen-emma-ch2.tsv">the gold standard
annotated version of chapter 2</a>.
</p>
<p>
Stanford NER CRF allows all properties to be specified on the
command line, but it is easier to use a <i>properties file</i>.  
Here is a simple 
properties file (pretty much like the one above!), but explanations for 
each line are in comments, specified by "#":
</p>
<blockquote><pre>
# location of the training file
trainFile = jane-austen-emma-ch1.tsv
# location where you would like to save (serialize) your
# classifier; adding .gz at the end automatically gzips the file,
# making it smaller, and faster to load
serializeTo = ner-model.ser.gz

# structure of your training file; this tells the classifier that
# the word is in column 0 and the correct answer is in column 1
map = word=0,answer=1

# This specifies the order of the CRF: order 1 means that features
# apply at most to a class pair of previous class and current class
# or current class and next class.
maxLeft=1

# these are the features we'd like to train with
# some are discussed below, the rest can be
# understood by looking at NERFeatureFactory
useClassFeature=true
useWord=true
# word character ngrams will be included up to length 6 as prefixes
# and suffixes only 
useNGrams=true
noMidNGrams=true
maxNGramLeng=6
usePrev=true
useNext=true
useDisjunctive=true
useSequences=true
usePrevSequences=true
# the last 4 properties deal with word shape features
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</pre>
</blockquote>
<p>
Here is that properties file as a downloadable link: <a
href="ner-example/austen.prop"><code>austen.prop</code></a>.
</p>
<p>
Once you have such a properties file, you can train a classifier with the command:
</p>
<p>
<code>
java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop
</code>
</p>
<p>
An NER model will then be serialized to the location specified in the
properties file (<code>ner-model.ser.gz</code>) 
once the program has completed. To check how well it works, you can run the test command:
</p>
<blockquote><code>
java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier ner-model.ser.gz -testFile jane-austen-emma-ch2.tsv
</code></blockquote>
<p>
In the output, the first column is the input tokens, the second column is the correct
(gold) answers, and the third column is the answer guessed by the
classifier. By looking at the output, you can see that the classifier
finds most of the person named entities but not all, mainly due to the
very small size of the training data (but also this is a fairly basic
feature set).
The code then evaluates the performance of the classifier for entity level
precision, recall, and F1. It gets 80.95% F1.  (A commonly used script
for NER evaluation is  the Perl script
<a href="http://www.cnts.ua.ac.be/conll2002/ner/bin/conlleval.txt">conlleval</a>,
but either it needs adapted or else the raw IO input format used here
needs to be mapped to IOB encoding for it to work correctly and give the
same answer.)
</p>
<p>
So how do you apply this to make your own non-example NER model? You
need 1) a training data source, 2) a properties file specifying the
features you want to use, and (optional, but nice) 3) a test file to
see how you're doing.  For the training data source, you need each
word to be on a separate line and annotated with the correct answer;
all columns must be tab-separated.  If you want to explicitly specify
more features for the word, you can add these in the file in a new
column and then put the appropriate structure of your file in the map
line in the properties file.  For example, if you added a third column
to your data with a new feature, you might write 
"map= word=0, answer=1, mySpecialFeature=2".  
</p><p>
Right now, most arbitrarily named features (like mySpecialFeature)
will <i>not</i> work without making modifications to the source code.
To see which features can already be attached to a CoreLabel, look
at <code>edu.stanford.nlp.ling.AnnotationLookup</code>.  There is a
table which creates a mapping between key and annotation type.  For
example, if you search in this file for <code>LEMMA_KEY</code>, you
will see that <code>lemma</code> produces a
<code>LemmaAnnotation</code>.  If you have added a new annotation, you
can add its type to this table, or you can use one of the known names
that already work, like <code>tag, lemma, chunk, web</code>.
</p><p>
If you modify AnnotationLookup, you need to read the data from the
column, translate it to the desired object type, and attach it to the
CoreLabel using a CoreAnnotation.  Quite a few CoreAnnotations are
provided in the class appropriately called CoreAnnotations.  If the
particular one you are looking for is not present, you can add a new
subclass by using one of the existing CoreAnnotations as an example.
</p><p>
If the feature you attached to the CoreLabel is not already used as a
feature in NERFeatureFactory, you will need to add code that extracts
the feature from the CoreLabel and adds it to the feature set.  Bear
in mind that features must have unique names, or they will conflict
with existing features, which is why we add markers such as "-GENIA",
"-PGENIA", and "-NGENIA" to our features.  As long as you choose a
unique marker, the feature itself can be any string followed by its
marker and will not conflict with any existing features.  Processing
is done using a bag of features model, with all of the features mixed
together, which is why it is important to not have any name conflicts.
</p><p>
Once you've annotated your data, you make a properties file with the
features you want.  You can use the example properties file, and refer
to the NERFeatureFactory for more possible features.  Finally, you can
test on your annotated test data as shown above or annotate more text
using the
<code>-textFile</code> command rather than <code>-testFile</code>. 
</p>
</li>

<li><h4><a name="d">How can I train an NER model using less memory?</a></h4>
<p>
Here are some tips on memory usage for CRFClassifier:
</p>
<ol>
<li>Ultimately, if you have tons of features and lots of classes, you
need to have lots of memory to train a CRFClassifier.  We frequently
train models that require several gigabytes of RAM and are used to
typing <code>java -mx4g</code>.</li>

<li>You can decrease the memory of the limited-memory quasi-Newton optimizer
(L-BFGS).  The optimizer maintains a number of past guesses which are
used to approximate the Hessian.  Having more guesses makes the estimate
more accurate, and optimization is faster, but the memory used by the
system during optimization is linear in the number of guesses.  This is
specified by the parameter <code>qnSize</code>.  The
default is 25.  Using 10 is perfectly adequate.  If you're short of
memory, things will still work with much smaller values, even just a
value of 2.</li>

<li>Use the flag <code>saveFeatureIndexToDisk = true</code>. The feature
names aren't actually needed while the core model estimation
(optimization) code is run.  This option saves them to a file
before the optimizer runs, enabling the memory they use to be freed, and
then loads the feature index from disk after optimization is finished.</li>

<li>Decrease the order of the CRF.  We usually use just first order CRFs
(<code>maxLeft=1</code> and no features that refer to the
<code>answer</code> class more than one away - it's okay to refer to
word features any distance away).  While the code supports arbitrary order CRFs,
building second, third, or fourth order CRFs will greatly increase
memory usage and normally isn't necessary.  Remember:
<code>maxLeft</code> refers to the size of the class contexts that your
features use (that is, it is one smaller than the clique size).  A first
order CRF can still look arbitrarily far to the left or right to get
information about the observed data context.</li>

<li>Decrease the number of features generated.  To see all the features
generated, you can set the property <code>printFeatures</code> to
<code>true</code>.  CRFClassifier will then write (potentially huge)
files in the current directory listing the features generated for each
token position.  Options that generate huge numbers of features include
<code>useWordPairs</code> and <code>useNGrams</code> when
<code>maxNGramLeng</code> is a large number.</li>

<li>Decrease the number of classes in your model.  This may or may not
be possible, depending on what your modeling requirements are.  But time
complexity is proportional to the number of classes raised to the clique
size.</li>

<li>Use the <code>flag useObservedSequencesOnly=true</code>.
This makes it so that you can only label adjacent words with label sequences
that were seen next to each other in the training data.  For some
kinds of data this actually gives better accuracy, for other kinds it is
worse.  But unless the label sequence patterns are dense, it will reduce
your memory usage.</li>

<li>Of course, shrinking the amount of training data will also reduce
  the memory needed, but isn't very desirable if you're trying to train
  the best classifier.  You might consider throwing out sentences with
  no entities in them, though.</li>

<li>If you're concerned about runtime memory usage, some of the above
items still apply (number of features and classes,
useObservedSequencesOnly, and order of the CRF), but in addition, you
can use the flag <code>featureDiffThresh</code>, for example
<code>featureDiffThresh=0.05</code>.  In training, CRFClassifier will
train one model, drop all the features with weight (absolute value)
beneath the given threshold, and then train a second model.  Training
thus takes longer, but the resulting model is smaller and faster at
runtime, and usually has very similar performance for a reasonable
threshold such as 0.05.</li>
</ol>

<li><h4><a name="mfiles">How do I train one model from multiple files?</a></h4>
<p>
Instead of setting the <code>trainFile</code> property or flag, set
the </code>trainFileList</code> property or flag.  Use a comma
separated list of files.
</li>

<li><h4><a name="i">What is the API for using CRFClassifier in a program?</a></h4>
<p>
Typically you would load a classifier from disk with the
<code>CRFClassifier.getClassifier()</code> method and then use it to
classify some text.  See the example <code>NERDemo</code> file.  The
two most flexible classification methods to call are called
<code>classify()</code>. These return a
<code>List&lt;CoreLabel&gt;</code>, or a list of those, and take the
same type or a String, respectively.  A <code>CoreLabel</code> has
everything you could need: the original token, its (Americanized, Penn
Treebank) normalized form used in the system, its begin and end character offsets, a
record of the whitespace around it, and the class assigned to the
token.  Print some out and have a look (<code>NERDEMO.java</code> will
print some for a sample sentence, if run with no arguments).  There are also a number of
other classification methods that take a String of text as input, and
provide various forms of user-friendly output.  The method
<code>classifyToCharacterOffsets</code> returns a list of triples of an
entity name and its begin and end character offsets.  The method
<code>classifyToString(String, String, boolean)</code> will return you a
String with NER-classified text in one of several formats (plain text or
XML) with 
or without token normalization and the preservation of spacing versus
tokenized.  One of the versions of it may well do what you would like to see.
Again, see <code>NERDemo</code>
for examples of the use of several (but not all) of these methods.
</li>

<li><h4><a name="c">Can I set the Stanford NER system up to allow single-jar 
deployment rather than it having to load NER models from separate files?</a></h4>
<p>
Yes!  But you'll need to make your own custom jar file.  If you insert
into the jar file an NER model with name <i>myModel</i> and you put it
inside the jar file under the <code>/classifiers/</code> path as
<code>/classifiers/myModel</code>, then you can load it when running
from a jar file with a command like:
</p>
<blockquote><code>
java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier
-loadClassifier classifiers/myModel -textFile sample.txt
</code></blockquote>

<li><h4><a name="cc">For our Web 5.0 system, can I run Stanford NER as a server/service/servlet?</a></h4>
<p>
Yes, you can.  You might look at
<code>edu.stanford.nlp.ie.NERServer</code> as an example of having the
CRFClassifier run on a socket and wait for text to annotate and
then returning the results.  Here's a complete Unix/Linux/Mac OS X
example, run from inside the folder of the distribution:
</p>
<blockquote><code>
$ cp stanford-ner.jar stanford-ner-with-classifier.jar <br>
$ jar -uf stanford-ner-with-classifier.jar classifiers/ner-eng-ie.crf-3-all2008.ser.gz <br>
$ java -mx500m -cp stanford-ner-with-classifier.jar edu.stanford.nlp.ie.NERServer
-port 9191 -loadJarClassifier ner-eng-ie.crf-3-all2008.ser.gz <br>
$ java -cp stanford-ner-with-classifier.jar
edu.stanford.nlp.ie.NERServer -port 9191 -client <br>
Input some text and press RETURN to NER tag it, or just RETURN to
finish. <br>
President Barack Obama met Fidel Castro at the United Nations in New
York. <br>
President/O Barack/PERSON Obama/PERSON met/O Fidel/PERSON Castro/PERSON
at/O the/O United/ORGANIZATION Nations/ORGANIZATION in/O New/LOCATION
York/LOCATION ./O
</code></blockquote>
<p>
With a bit of work, we're sure you can
adapt that example to work in a REST, SOAP, AJAX, or whatever system.  If not,
pay us a lot of money, and we'll work it out for you.  Or if you don't
want to do that, maybe look at 
<a href="https://github.com/dat/stanford-ner">what Hat Doang did</a>. 
An adaptation of that code now runs
<a href="http://nlp.stanford.edu:8080/ner/">our online demo</a>.
</p>
</li>

<li><h4><a name="e">Why do I get compilation errors when I try to compile
recent versions of the source code (such as the 2008-05-07 version)?</a></h4>
<p>
In recent versions of our NER code, we use the typesafe heterogeneous
container pattern that Josh Bloch has talked about in various places such
as <a href="http://developers.sun.com/learning/javaoneonline/2007/pdf/TS-2689.pdf">this
talk</a>. It's neat but somewhat stresses the implementation of generic types
in Java.  The code is correct and should compile okay.  It does compile okay
with current versions of Sun javac v1.5 or v1.6 and with the current
version of the Eclipse compiler.  If it doesn't compile for you, you
should upgrade your Java compiler or complain to the person who makes it.
</p>
</li>

<li><h4><a name="h">How can I use the version of <code>NERDemo.java</code>
included in the 2008-05-07 version of the software?</a></h4>
<p>
In this release (only) we made a booboo, and didn't update the
<code>NERDemo.java</code> file to correspond to changes in the main
code, and the supplied code doesn't compile.
Here is a downloadable version of <a
href="ner-example/stanford-ner-2008-05-07/NERDemo.java"><code>NERDemo.java</code></a>,
which will work with that version.  The commands you need to use it are
:
<blockquote><code>
javac -cp "stanford-ner.jar:." NERDemo.java <br>
java -cp "stanford-ner.jar:."  NERDemo <br>
java -mx400m -cp "stanford-ner.jar:."  NERDemo classifiers/ner-eng-ie.crf-3-all2008-distsim.ser.gz myFile.txt
</code></blockquote>
(These are the commands for Linux or the Mac OS X command-line; for
Windows, replace the colons above with semicolons and the slash with a backslash.)
</p>
</li>


<li><h4><a name="j">What options are available for formatting the output of the classifier?</a></h4>
<p>
Several options are available from the command-line for determining the output format of the classifier.  
You can choose an <code>outputFormat</code> of <code>xml</code>, <code>inlineXML</code>,
or <code>slashTags</code> (the default).  See the example of each below (these are <code>bash</code>
shell command lines, the last bit of which suppresses message printing, so you can see just the output).
Even more power is available if you are using the API.
The <code>classifier.classifyToString(String text, String outputFormat, boolean preserveSpaces)</code>
method supports 6 output styles (of which 3 are available with the <code>outputFormat</code> property:
the XML output options preserve spaces, but the slash tags one doesn't).  Even more flexibility can be 
obtained by using other of the <code>classify.*</code> methods in the API.  These will return 
classified versions of the input, which you can print out however your heart desires!
There are also methods like
<code>classifyToCharacterOffsets(String)</code> which returns just the
entity spans.
See the examples in <code>NERDemo.java</code>.</p>

<pre>
$ cat PatrickYe.txt
I complained to Microsoft about Bill Gates.
  They     told me to see the mayor of New York.
$
$ java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/ner-eng-ie.crf-3-all2008-distsim.ser.gz -textFile PatrickYe.txt -outputFormat slashTags  2> /dev/null
I/O complained/O to/O Microsoft/ORGANIZATION about/O Bill/PERSON Gates/PERSON ./O They/O told/O me/O to/O see/O the/O mayor/O of/O New/LOCATION York/LOCATION ./O 
$
$ java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/ner-eng-ie.crf-3-all2008-distsim.ser.gz -textFile PatrickYe.txt -outputFormat inlineXML
I complained to &lt;ORGANIZATION&gt;Microsoft&lt;/ORGANIZATION&gt; about &lt;PERSON&gt;Bill Gates&lt;/PERSON&gt;.
  They     told me to see the mayor of &lt;LOCATION&gt;New York&lt;/LOCATION&gt;.
$
$ java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/ner-eng-ie.crf-3-all2008-distsim.ser.gz -textFile PatrickYe.txt -outputFormat xml
&lt;wi num="0" entity="O"&gt;I&lt;/wi&gt; &lt;wi num="1" entity="O"&gt;complained&lt;/wi&gt; &lt;wi num="2" entity="O"&gt;to&lt;/wi&gt; &lt;wi num="3" entity="ORGANIZATION"&gt;Microsoft&lt;/wi&gt; &lt;wi num="4" entity="O"&gt;about&lt;/wi&gt; &lt;wi num="5" entity="PERSON"&gt;Bill&lt;/wi&gt; &lt;wi num="6" entity="PERSON"&gt;Gates&lt;/wi&gt;&lt;wi num="7" entity="O"&gt;.&lt;/wi&gt;
  &lt;wi num="0" entity="O"&gt;They&lt;/wi&gt;     &lt;wi num="1" entity="O"&gt;told&lt;/wi&gt; &lt;wi num="2" entity="O"&gt;me&lt;/wi&gt; &lt;wi num="3" entity="O"&gt;to&lt;/wi&gt; &lt;wi num="4" entity="O"&gt;see&lt;/wi&gt; &lt;wi num="5" entity="O"&gt;the&lt;/wi&gt; &lt;wi num="6" entity="O"&gt;mayor&lt;/wi&gt; &lt;wi num="7" entity="O"&gt;of&lt;/wi&gt; &lt;wi num="8" entity="LOCATION"&gt;New&lt;/wi&gt; &lt;wi num="9" entity="LOCATION"&gt;York&lt;/wi&gt;&lt;wi num="10" entity="O"&gt;.&lt;/wi&gt;
</pre>
</li>


<!--  Gazettes don't work with the currently distributed version
<li><h4><a name="f">Can you use gazettes (name lists) with Stanford NER?</a></h4>
<p>
The Stanford NER Classifier supports gazettes (name lists).  However,
gazettes can only be used at runtime if a gazette and gazette features
were enabled at training time (so the system can estimate the
reliability of the gazette).  In practice, you can then post-hoc add some things
to the gazette before running the NER Classifier, these terms will be
recognized, and things will all work out okay.
</p>
<p>
However, the default supplied NER model does not provide or use any
gazette, so adding a gazette to it will do no good.  (That is, the
gazette features will have zero weight in the classifier.)  
</p>
<p>
Here's how to define a gazette for your own NER model, extending the
example above.  Here is a small gazette for <i>Emma</i>: 
<a href="ner-example/emma-gazette.txt"><code>emma-gazette.txt</code></a>, which
contains:
</p>
<blockquote>
PERS Emma Woodhouse <br>
PERS Jane Austen <br>
PERS Miss Taylor <br>
PERS Mr. Frank Churchill <br>
PERS Mrs. and Miss Bates <br>
</blockquote>
<p>
We can train and test again with this gazette:
</p>
<blockquote><pre>
java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop -gazette emma-gazette.txt -cleanGazette
java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier ner-model.ser.gz -gazette emma-gazette.txt -testFile jane-austen-emma-ch2.tsv
</pre></blockquote>
</li>
-->

<li><h4><a name="k">Why can't I reproduce the results of your CoNLL 2003
system?  Or: How do I get better CoNLL 2003 English performance?</a></h4>
<p>
The classifier used for the CoNLL 2003 shared task <i>isn't</i> the same as the  
one we distribute.  The former was a 3rd order CMM (MEMM) classifier,  
while the latter is a 1st order CRF. So you shouldn't have any  
detailed assumptions about being able to replicate the results of that  
paper.  (But the features used in that paper are fairly carefully  
documented, and an earlier version of NERFeatureFactory was used in  
that system, so between the two you should be able to work out pretty  
exactly the features that paper used.)
</p>
<p>
For our distributed CRF (2009 version), the CoNLL classifier that we
distribute wasn't very well tuned.  Here are two better models:
</p>
<blockquote>
<a href="http://nlp.stanford.edu/software/conll.closed.iob2.crf.ser.gz">http://nlp.stanford.edu/software/conll.closed.iob2.crf.ser.gz</a>
<br>
<a href="http://nlp.stanford.edu/software/conll.distsim.iob2.crf.ser.gz">http://nlp.stanford.edu/software/conll.distsim.iob2.crf.ser.gz</a>
</blockquote>
<p>
These haven't been tested on the CoNLL English testb data, but based on  
their performance on the testa data (91.3 and 93.0 F1), we'd predict  
that they'd get about 85.3 and 87.5 F1 respectively on the testb data.  
So, their performance is within range of previous CoNLL 2003 results.
The first is trained only on the training data, while the second makes
use of distributional similarity features trained on a larger data set.
Also, these models are IOB2 classifiers unlike the IO classifiers which we
distribute with the software.
</p>
</li>


<li><h4><a name="l">How do I get German NER working right?  My characters
    are messed up.</a></h4>
<p>
This is almost always a character encoding issue.  You need to know
how your German text files are encoded (normally either one of the
traditional 8-bit encodings ISO-8859-1 or ISO-8859-15, or Unicode
utf-8).  Then you either need to know the default platform character
encoding of your computer or to override it (these days the default
encoding is normally utf-8, unless you're
on Mac OS X, where Mac OS X Java <i>still</i> uses the 8-bit MacRoman
encoding as its default, despite most of the rest of the OS using utf-8).
You can override the
encoding of read and written files with the <code>-inputEncoding</code>
and <code>-outputEncoding</code> flags.
</p>
</li>

<li><h4><a name="mem">What is the asymptotic memory growth of the classifier?</a></h4>
<p>
The asymptotic memory growth in number of states <code>S</code>
depends on the order of the CRF.  If the CRF is order <code>n</code>,
then the memory growth will be <code>S^(n+1)</code>.
</p>
</li>

<li><h4><a name="extend">Can an existing model be extended?</a></h4>
<p>
Unfortunately, no.
</p>
</li>

<li><h4><a name="data">Can you release the data used to build the publicly released models</a></h4>
<p>
Unfortunately, no.  The licenses that come with that data do not allow
for redistribution.  However, you can see
the <a href="http://nlp.stanford.edu/software/CRF-NER.shtml#Models">models</a>
section of the NER page for a brief description of what the data sets
were.
</p>
</li>


<li><h4><a name="deterministic">Is the NER deterministic?  Why do the results change for the same data?</a></h4>

<p>
Yes, the underlying CRF is deterministic.  If you apply the NER to the
same sentence more than once, though, it is possible to get different
answers the second time.  The reason for this is the NER remembers
whether it has seen a word in lowercase form before.
</p><p>
The exact way this is used as a feature is in the word shape feature,
which treats words such as "Brown" differently if it has or has not
seen "brown" as a lowercase word before.  If it has, the word shape
will be "Initial upper, have seen all lowercase", and if it has not,
the word shape will be "Initial upper, have not seen all lowercase".
</p>

</li>

</ol>

<p>
You can discuss other topics with Stanford NER developers and users by 
<a
href="https://mailman.stanford.edu/mailman/listinfo/java-nlp-user">joining
the <code>java-nlp-user</code> mailing list</a>
(via a webpage).  Or you can send other questions and feedback to 
<a href="mailto:java-nlp-support@lists.stanford.edu"><code>java-nlp-support@lists.stanford.edu</code></a>.  
</p>


<!--#include virtual="/footer.html" -->

