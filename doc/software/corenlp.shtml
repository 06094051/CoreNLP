<!--#include virtual="/header.html" -->

<center>
<h2>Stanford CoreNLP</h2>
<h3>A Suite of Core NLP Tools</h3>
</center>

<center><p><a href="#About">About</a> |
<a href="#Download">Download</a> |
<a href="#Usage">Usage</a> |
<a href="#newannotators">Adding Annotators</a> |
<a href="#caseless">Caseless Models</a> |
<a href="#Extensions">Extensions</a> |
<a href="#Questions">Questions</a> |
<a href="#Mail">Mailing lists</a> |
<a href="#Demo">Online demo</a> |
<a href="corenlp-faq.shtml">FAQ</a> |
<a href="#History">Release history</a>
</p></center>


<h3><a name="About">About</a></h3>

<p>
Stanford CoreNLP provides a set of natural language analysis
tools which can take raw English language text input and give the base
forms of words, their parts of speech, whether they are names of
companies, people, etc., normalize dates, times, and numeric quantities,
and mark up the structure of sentences in terms of
phrases and word dependencies, and indicate which noun phrases refer to
the same entities.  Stanford CoreNLP is an integrated framework, which
make it very easy to apply a bunch of language analysis tools to a piece
of text. Starting from plain text, you can run all the tools on it with
just 
two lines of code. Its analyses provide the foundational building blocks for
higher-level and domain-specific text understanding applications.
</p><p>
Stanford CoreNLP integrates all our NLP tools,
including <a href="tagger.shtml">the part-of-speech (POS) tagger</a>, <a
href="CRF-NER.shtml">the named entity recognizer
    (NER)</a>, <a href="lex-parser.shtml">the parser</a>,
and <a href="dcoref.shtml">the coreference resolution system</a>, and
provides model files for analysis of English. The goal of this project
is to enable people to quickly and painlessly get complete linguistic
annotations of natural language texts. It is designed to be highly
flexible and extensible.  With a single option you can change which
tools should be enabled and which should be disabled. 
</p><p>
The Stanford CoreNLP code is written in Java and licensed under the
<a href="http://www.gnu.org/licenses/gpl-2.0.html">GNU
General Public License</a></b> (v2 or later).  Source is included.
Note that this is the <i>full</i> GPL,
which allows many free uses, but not its use in distributed 
<a href="http://www.gnu.org/licenses/gpl-faq.html#GPLInProprietarySystem">proprietary
software</a>.  The download is 259 MB and requires Java 1.6+.
</p>

<br>
<h3><a name="Download">Download</a></h3>

<CENTER>
<b><font color="#a40526"><a href="stanford-corenlp-full-2012-11-12.zip">
Download Stanford CoreNLP version 1.3.4</a></font></b>. 
</CENTER>
<p>
Or you can find it on 
<a href="http://mvnrepository.com/artifact/edu.stanford.nlp/stanford-corenlp">Maven
Central</a>.


<br>
<h3><a name="Usage">Usage</a></h3>

<p><b><u>Parsing a file and saving the output as XML</u></b></p>

<p>
Before using Stanford CoreNLP, it is usual to create a configuration
file (a Java Properties file). Minimally, this file should contain the "annotators" property, which contains a comma-separated list of Annotators to use. For example, the setting below enables: tokenization, sentence splitting (required by most Annotators), POS tagging, lemmatization, NER, syntactic parsing, and coreference resolution.
</p>
<pre>
annotators = tokenize, ssplit, pos, lemma, ner, parse, dcoref
</pre>
<p>
However, if you just want to specify one or two properties, you can
instead place them on the command line.
</p>

<p>
To process one file using Stanford CoreNLP, use the following sort of command line (adjust the JAR file date extensions to your downloaded release):
</p>
<pre>
java -cp stanford-corenlp-YYYY-MM-DD.jar:stanford-corenlp-YYYY-MM-DD-models.jar:xom.jar:joda-time.jar -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP &#91; -props &lt;YOUR CONFIGURATION FILE&gt; &#93; -file &lt;YOUR INPUT FILE&gt;
</pre>
In particular, to process the included sample
file <code>input.txt</code> you can use this command in the distribution
directory:
<pre>
java -cp stanford-corenlp-1.3.4.jar:stanford-corenlp-1.3.4-models.jar:xom.jar:joda-time.jar:jollyday.jar
-Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt
</pre>
Notes:
<ul>
<li>Stanford CoreNLP requires Java version 1.6 or higher.
<li><code>-Xmx3g</code> specifies the amount of RAM that Java will reserve.
On a 64-bit machine, Stanford CoreNLP typically requires 3GB to run
(and it may need even more, depending on the size of the document to
parse). It is not recommended to run this program on a 32 bit
machines, as 32 bit Java will not allocate even as much as 2G.  This
is especially a problem on Windows machines.</li>
<li>The above command works for Mac OS X or Linux.  For Windows, the
colons (:) separating the jar files need to be semi-colons (;).  And, if you
are not sitting in the distribution directory, you'll also need to
include a path to the files before each.
<li>The -annotators argument is actually optional. If you leave it out, the code uses a built in properties file, 
which enables the following annotators: tokenization and sentence splitting, POS tagging, lemmatization, NER, parsing, and 
coreference resolution (that is, what we used in this example).</li>
<li>Processing a short text like this is very inefficient.  It
  takes about two minutes to load everything before processing
  begins. You should batch your processing.</li>
</ul>

<p>
If you want to process a list of files use the following command line:
</p>
<pre>
java -cp stanford-corenlp-YYYY-MM-DD.jar:stanford-corenlp-models-YYYY-MM-DD.jar:xom.jar:joda-time.jar -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP &#91; -props &lt;YOUR CONFIGURATION FILE&gt; &#93; -filelist &lt;YOUR LIST OF FILES&gt;
</pre>
<p>
where the <code>-filelist</code> parameter points to a file whose content lists all files to be processed (one per line).
</p>

<p>
Note that the <code>-props</code> parameter is optional -- by default, it
will search for <code>StanfordCoreNLP.properties</code> in your classpath
and use the defaults included in the distribution.
</p>

<p>
By default, output files are written to the current directory.
You may specify an alternate output directory with the flag
<code>-outputDirectory</code>.  Output filenames are the same as input
filenames but with <code>-outputExtension</code> added them (<code>.xml</code>
by default).  It will overwrite (clobber) output files by default.
Pass <code> -noClobber</code> to avoid this behavior.  Additionally, if you'd
rather it replace the extension with the <code>-outputExtension</code>, pass
the <code>-replaceExtension</code> flag.  This will result in filenames like
<code>test.xml</code> instead of <code>test.txt.xml</code> (when given <code>test.txt</code>
as an input file).
</p>

<p>
For each input file, Stanford CoreNLP generates one XML file with all relevant annotation. For example, for the above configuration and a file containing the text below:
</p>
<pre>
Stanford University is located in California. It is a great university.
</pre>
<p>
Stanford CoreNLP generates the 
<a href="corenlp_output.html">following output</a>, with the 
<a href="corenlp_xml_description.shtml">following attributes</a>.
</p>

<p>
Note that the XML output uses the CoreNLP-to-HTML.xsl stylesheet file, which can be downloaded from <a href="CoreNLP-to-HTML.xsl">here</a>.
This stylesheet enables human-readable display of the above XML content. For example, the previous example should be displayed like <a href="example.xml">this</a>.
</p>

<p>
Stanford CoreNLP also has the ability to remove most XML from a document before processing it.  (CDATA is not correctly handled.)  For example, if run with the annotators
</p>

<pre>
annotators = tokenize, cleanxml, ssplit, pos, lemma, ner, parse, dcoref
</pre>

<p>
and given the text
</p>

<pre>
&lt;xml&gt;Stanford University is located in California. It is a great university.&lt;/xml&gt;
</pre>

Stanford CoreNLP generates the <a href="corenlp_output2.html">following output</a>.  Note that the only difference between this and <a href="corenlp_output.html">the original output</a> is the change in CharacterOffsets.

<br>
<p><b><u>Using the Stanford CoreNLP API</u></b></p>

<p>
The backbone of the CoreNLP package is formed by two classes: Annotation and Annotator. Annotations are the data structure which hold the results of annotations. Annotations are basically maps, from keys to bits of the annotation, such as the parse, the part-of-speech tags, or named entity tags.
Annotators are a lot like functions, except that they operate over Annotations instead of Objects. They do things like tokenize, parse, or NER tag sentences. 
Annotators and Annotations are integrated by AnnotationPipelines, which
create sequences of generic Annotators. Stanford CoreNLP inherits from the AnnotationPipeline class, and is customized with NLP Annotators.
</p>

<p>
The table below summarizes the Annotators currently supported and the Annotations that they generate. 

<p>
<table width="90%" align="center" style="border: 1px dashed; border-color:#777777" cellspacing="15">
<tr><td align=center width="10%"><b>Property name</b></td><td align=center width="20%"><b>Annotator class name</b></td><td align=center width="20%"><b>Generated Annotation</b></td><td align=left width="50%"><b>Description</b></td></tr>
<tr><td align=center>tokenize</td><td align=center>PTBTokenizerAnnotator</td><td align=center>TokensAnnotation (list of tokens), and CharacterOffsetBeginAnnotation, CharacterOffsetEndAnnotation, TextAnnotation (for each token) </td><td>
Tokenizes the text. This component started as a PTB-style tokenizer, but was extended since then to handle noisy and web text. The tokenizer saves the character offsets of each token in the input text, as CharacterOffsetBeginAnnotation and CharacterOffsetEndAnnotation.
</td></tr>
<tr><td align=center>cleanxml</td><td align=center>CleanXmlAnnotator</td><td align=center>XmlContextAnnotation</td><td>Remove xml tokens from the document</td></tr>
<tr><td align=center>ssplit</td><td align=center>WordToSentenceAnnotator</td><td align=center>SentencesAnnotation</td><td>Splits a sequence of tokens into sentences.</td></tr>
<tr><td align=center>pos</td><td align=center>POSTaggerAnnotator</td><td align=center>PartOfSpeechAnnotation</td><td>Labels tokens with their POS tag. For more details see <a href="tagger.shtml">this page</a>.</td></tr>
<tr><td align=center>lemma</td><td align=center>MorphaAnnotator</td><td align=center>LemmaAnnotation</td><td>Generates the word lemmas for all tokens in the corpus.</td></tr>
<tr><td align=center>ner</td><td align=center>NERClassifierCombiner</td><td align=center>NamedEntityTagAnnotation and NormalizedNamedEntityTagAnnotation</td><td>Recognizes named (PERSON, LOCATION, ORGANIZATION, MISC) and numerical entities (DATE, TIME, MONEY, NUMBER). Named entities are recognized using a combination of three CRF sequence taggers trained on various corpora, such as ACE and MUC. Numerical entities are recognized using a rule-based system. Numerical entities that require normalization, e.g., dates, are normalized to NormalizedNamedEntityTagAnnotation. For more details on the CRF tagger see <a href="CRF-NER.shtml">this page</a>.</td></tr>
<tr><td align=center>regexner</td><td align=center>RegexNERAnnotator</td><td align=center>NamedEntityTagAnnotation</td><td>Implements a rule-based NER using Java regular expressions. The goal of this Annotator is to provide a simple framework to incorporate NE labels that are not annotated in traditional NL corpora. For example, the default list of regular expressions that we distribute recognizes ideologies (IDEOLOGY), nationalities (NATIONALITY), religions (RELIGION), and titles (TITLE).</td></tr>
<tr><td align=center>truecase</td><td align=center>TrueCaseAnnotator</td><td align=center>TrueCaseAnnotation and TrueCaseTextAnnotation</td><td>Recognizes the true case of tokens in text where this information was lost, e.g., all upper case text. This is implemented with a discriminative model implemented using a CRF sequence tagger. The true case label, e.g., INIT_UPPER is saved in TrueCaseAnnotation. The token text adjusted to match its true case is saved as TrueCaseTextAnnotation.</td></tr>
<tr><td align=center>parse</td><td align=center>ParserAnnotator</td><td align=center>TreeAnnotation, BasicDependenciesAnnotation, CollapsedDependenciesAnnotation, CollapsedCCProcessedDependenciesAnnotation</td><td>Provides full syntactic analysis, using both the constituent and the dependency representations. 
The constituent-based output is saved in TreeAnnotation. We generate three dependency-based outputs, as follows: basic, uncollapsed dependencies, saved in BasicDependenciesAnnotation; collapsed dependencies saved in CollapsedDependenciesAnnotation; and collapsed dependencies with processed coordinations, in CollapsedCCProcessedDependenciesAnnotation. Most users of our parser will prefer the latter representation.
For more details on the parser, please see <a href="lex-parser.shtml">this page</a>. For more details about the dependencies, please refer to <a href="stanford-dependencies.shtml">this page</a>.
</td></tr>
<tr><td align=center>dcoref</td><td align=center>DeterministicCorefAnnotator</td><td align=center>CorefChainAnnotation</td><td>Implements both pronominal and nominal coreference resolution. The entire coreference graph (with head words of mentions as nodes) is saved in CorefChainAnnotation. For more details on the underlying coreference resolution algorithm, see <a href="dcoref.shtml">this page</a>.</td></tr>
</table>

<p>
Depending on which annotators you use, please cite the corresponding papers on: <a href="tagger.shtml">POS tagging</a>, <a href="CRF-NER.shtml">NER</a>, <a href="lex-parser.shtml">parsing</a>, or <a href="dcoref.shtml">coreference resolution</a>.

<p>
To construct a Stanford CoreNLP object from a given set of properties, use <code>StanfordCoreNLP(Properties props)</code>. This method creates the pipeline using the annotators given in the "annotators" property (see above for an example setting). The complete list of accepted annotator names is listed in the first column of the table at the top of this page. To parse an arbitrary text, use the <code>annotate(Annotation document)</code> method. 

<p>
The code below shows how to create and use a Stanford CoreNLP object:

<p>
<pre>
    // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution 
    Properties props = new Properties();
    props.put("annotators", "tokenize, ssplit, pos, lemma, ner, parse, dcoref");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    
    // read some text in the text variable
    String text = ... // Add your text here!
    
    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);
    
    // run all Annotators on this text
    pipeline.annotate(document);
    
    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
    
    for(CoreMap sentence: sentences) {
      // traversing the words in the current sentence
      // a CoreLabel is a CoreMap with additional token-specific methods
      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
        // this is the text of the token
        String word = token.get(TextAnnotation.class);
        // this is the POS tag of the token
        String pos = token.get(PartOfSpeechAnnotation.class);
        // this is the NER label of the token
        String ne = token.get(NamedEntityTagAnnotation.class);       
      }

      // this is the parse tree of the current sentence
      Tree tree = sentence.get(TreeAnnotation.class);

      // this is the Stanford dependency graph of the current sentence
      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
    }

    // This is the coreference link graph
    // Each chain stores a set of mentions that link to each other,
    // along with a method for getting the most representative mention
    // Both sentence and token offsets start at 1!
    Map&lt;Integer, CorefChain&gt; graph = 
      document.get(CorefChainAnnotation.class);
</pre>

<p><b><u>Annotator options</u></b>

<p>
While all Annotators have a default behavior that is likely to be sufficient for the majority of users, most Annotators take additional options that can be passed as Java properties in the configuration file. We list below the configuration options for all Annotators:

<p>
<b>tokenize</b>:<br>
<ul>
<li>tokenize.whitespace: if set to true, separates words only when
whitespace is encountered.
<li>tokenize.options: Accepts the options of <code>PTBTokenizer</code>
  for example, things like "americanize=false" or
  "strictTreebank3=true,untokenizable=allKeep".
</ul>

<b>cleanxml</b>:<br>
<ul>
<li>clean.xmltags: Discard xml tag tokens that match this regular expression.  For example, .* will discard all xml tags.
<li>clean.sentenceendingtags: treat tags that match this regular expression as the end of a sentence.  For example, p will treat &lt;p&gt; as the end of a sentence.
<li>clean.allowflawedxml: if this is true, allow errors such as unclosed tags.  Otherwise, such xml will cause an exception.
<li>clean.datetags: a regular expression that specifies which tags to treat as the reference date of a document.  Defaults to datetime|date
</ul>

<b>ssplit</b>:<br>
<ul>
<li>ssplit.eolonly: only split sentences on newlines.  Works well in
conjunction with "-tokenize.whitespace true", in which case
StanfordCoreNLP will split one sentence per line, only separating
words on whitespace.
<li>ssplit.isOneSentence: each document is to be treated as one
sentence, no splitting at all.
</ul> 

<b>pos</b>:<br>
<ul>
<li>pos.model: POS model to use. There is no need to explicitly set this option, unless you want to use a different POS model (for advanced developers only). By default, this is set to the POS model included in the stanford-corenlp-models JAR file.
<li>pos.maxlen: Maximum sentence size for the POS sequence tagger. Any sentence larger than this is split in smaller sentences before tagging. Useful to control the speed of the tagger on noisy text without punctuation marks.
</ul>

<b>ner</b>:<br>
<ul>
<li>ner.useSUTime: Whether or not to use sutime.  On by default in the version which includes sutime, off by default in the version that doesn't.
<li>ner.model: NER model(s) in a comma separated list to use instead of the default models.  By default, the models used will be the 3class, 7class, and MISCclass models, in that order.
<li>ner.applyNumericClassifiers: Whether or not to use numeric classifiers, including <a href="sutime.shtml">SUTime</a>.  These are hardcoded for English, so if using a different language, this should be set to false.
</ul>

<b>regexner</b>:<br>
<ul>
<li>regexner.mapping: file that stores rules, i.e., the mapping from regular expressions to NE classes. The format is one rule per line; each rule has two mandatory tokens separated by one tab. The first token stores the Java regular expression. The second token points to the NE class to assign when the regular expression matches one or a sequence of tokens. An optional third rule token indicates which regular named entity types can be overwritten by the current rule. For example, the rule "U\.S\.A\.       COUNTRY LOCATION" marks the token "U.S.A." as a COUNTRY, overwriting the previous LOCATION label (if it exists).
<li>regexner.ignorecase: if set to true, matching will be case insensitive. Default value is false.
</ul>

<b>parse</b>:<br>
<ul>
<li>parse.model: parsing model to use. There is no need to explicitly set this option, unless you want to use a different parsing model (for advanced developers only). By default, this is set to the parsing model included in the stanford-corenlp-models JAR file.
<li>parse.maxlen: if set, the annotator parses only sentences shorter (in terms of number of tokens) than this number. For longer sentences, the parser creates a flat structure, where every token is assigned to the non-terminal X. This is useful when parsing noisy web text, which may generate arbitrarily long sentences. By default, this option is not set.
</ul>

<b>dcoref</b>:<br>
<ul>
<li>dcoref.sievePasses: list of sieve modules to enable in the system, specified as a comma-separated list of class names. By default, this property is set to include all available sieve passes: "edu.stanford.nlp.dcoref.sievepasses.MarkRole, edu.stanford.nlp.dcoref.sievepasses.ExactStringMatch, edu.stanford.nlp.dcoref.sievepasses.RelaxedExactStringMatch, edu.stanford.nlp.dcoref.sievepasses.PreciseConstructs, edu.stanford.nlp.dcoref.sievepasses.StrictHeadMatch1, edu.stanford.nlp.dcoref.sievepasses.StrictHeadMatch2, edu.stanford.nlp.dcoref.sievepasses.StrictHeadMatch3, edu.stanford.nlp.dcoref.sievepasses.RelaxedHeadMatch, edu.stanford.nlp.dcoref.sievepasses.PronounMatch".  The default value can be found in Constants.SIEVEPASSES.
<li>dcoref.demonym: list of demonyms from <a href="http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names">http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names</a>. The format of this file is: location TAB singular gentilic form TAB plural gentilic form, e.g., "Algeria Algerian Algerians".
<li>dcoref.animate and dcoref.inanimate: lists of animate/inanimate words, from (Ji and Lin, 2009). The format is one word per line.
<li>dcoref.male, dcoref.female, dcoref.neutral: lists of words of male/female/neutral gender, from (Bergsma and Lin, 2006) and (Ji and Lin, 2009). The format is one word per line.
<li>dcoref.plural and dcoref.singular: lists of words that are plural or singular, from (Bergsma and Lin, 2006). The format is one word per line. All the above dictionaries are already set to the files included in the stanford-corenlp-models JAR file, but they can easily be adjusted to your needs by setting these properties.
<li>oldCorefFormat: produce a CorefGraphAnnotation, the output format used in releases v1.0.3 or earlier.  Note that this uses quadratic memory rather than linear.
</ul>

<p><b><u>SUTime</u></b></p>

<p>
StanfordCoreNLP includes <a href="sutime.shtml">SUTime</a>, Stanford's temporal expression
recognizer. SUTime is transparently called from the "ner" annotator,
so no configuration is necessary. Furthermore, the "cleanxml"
annotator now extracts the reference date for a given XML document, so
relative dates, e.g., "yesterday", are transparently normalized with
no configuration necessary.
</p><p>
SUTime supports the same annotations as before, i.e.,
NamedEntityTagAnnotation is set with the label of the numeric entity (DATE,
TIME, DURATION, MONEY, PERCENT, or NUMBER) and
NormalizedNamedEntityTagAnnotation is set to the value of the normalized
temporal expression. Note that NormalizedNamedEntityTagAnnotation now
follows the TIMEX3 standard, rather than Stanford's internal representation,
e.g., "2010-01-01" for the string "January 1, 2010", rather than "20100101".
</p><p>
Also, SUTime now sets the TimexAnnotation key to an
edu.stanford.nlp.time.Timex object, which contains the complete list of
TIMEX3 fields for the corresponding expressions, such as "val", "alt_val",
"type", "tid". This might be useful to developers interested in recovering
complete TIMEX3 expressions.
</p><p>

Reference dates are by default extracted from the "datetime" and
"date" tags in an xml document.  To set a different set of tags to
use, use the clean.datetags property.  When using the API, reference
dates can be added to an <code>Annotation</code> via
<code>edu.stanford.nlp.ling.CoreAnnotations.DocDateAnnotation</code>,
although note that when processing an xml document, the cleanxml
annotator will overwrite the <code>DocDateAnnotation</code> if
"datetime" or "date" are specified in the document.
</p>

<p><b><u>TokensRegex</u></b></p>
<p>
StanfordCoreNLP includes <a href="tokensregex.shtml">TokensRegex</a>, a framework for defining regular expressions over 
text and tokens, and mapping matched text to semantic objects.
</p>

<p><b><u>Javadoc</u></b>

<p>
More information is available in the javadoc: 
<a href="http://www-nlp.stanford.edu/nlp/javadoc/javanlp/">
  Stanford Core NLP Javadoc</a>.

<h3><a name="newannotators">Adding a new annotator</a></h3>
<p>
StanfordCoreNLP also has the capacity to add a new annotator by
reflection without altering the code in StanfordCoreNLP.java.  To
create a new annotator, extend the class
edu.stanford.nlp.pipeline.Annotator and define a constructor with the
signature (String, Properties).  Then, add the property
customAnnotatorClass.FOO=BAR to the properties used to create the
pipeline.  If FOO is then added to the list of annotators, the class
BAR will be created, with the name used to create it and the
properties file passed in.
</p>

<h3><a name="caseless">Caseless Models</a></h3>
<p>
It is possible to run StanfordCoreNLP with tagger, parser, and NER
models that ignore capitalization.  In order to do this, download the
<a href="stanford-corenlp-caseless-2012-11-09-models.jar">caseless
models</a> package.  Be sure to include the path to the case
insensitive models jar in the <code>-cp</code> classpath flag as well.
Then, set properties which point to these models as follows:
<br>
<code>
-pos.model edu/stanford/nlp/models/pos-tagger/english-caseless-left3words-distsim.tagger <br>
-parse.model edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz <br>
-ner.model.3class edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz <br>
-ner.model.7class edu/stanford/nlp/models/ner/english.muc.7class.caseless.distsim.crf.ser.gz <br>
-ner.model.MISCclass edu/stanford/nlp/models/ner/english.conll.4class.caseless.distsim.crf.ser.gz <br>
</code>
</p>

<h3><a name="Extensions">Extensions: Packages by others using Stanford CoreNLP</a></h3>

<p>
<a href="http://code.google.com/p/cleartk/source/browse/trunk/cleartk-stanford-corenlp">cleartk-stanford-corenlp</a> is a UIMA wrapper for Stanford
CoreNLP built by Steven Bethard in the context of the 
<a href="http://code.google.com/p/cleartk/">ClearTK</a> toolkit.
</p>

<p>
<a href="https://code.google.com/p/dkpro-core-gpl/">dkpro-core-gpl</a>
is a collection of NLP components, principally Stanford CoreNLP,
wrapped as UIMA components, based on work at the Ubiquitous Knowledge
Processing Lab (UKP) at the Technische Universität Darmstadt.  It is
part of the
<a href="http://www.ukp.tu-darmstadt.de/research/current-projects/dkpro/">DKPro</a>
project.
See also the <a href="http://code.google.com/p/dkpro-core-asl/wiki/WikiEntryPage">DKPro
Core wiki</a>. 
</p>

<p>
<a href="https://github.com/dasmith/stanford-corenlp-python">
Python wrapper including JSON-RPC server</a> by Dustin Smith.
</p>

<p>
<a href="https://metacpan.org/module/Lingua::StanfordCoreNLP">Perl
  wrapper</a> by Kalle Räisänen.


<p>
<a href="https://github.com/louismullie/stanford-core-nlp">
Ruby bindings</a> by Louis Mullie.
</p>

<p>
<a href="https://github.com/guokr/stan-cn-nlp">
Wrapper for each of Stanford's Chinese tools</a> by Mingli Yuan.
</p>

<h3><a name="Questions">Questions</a></h3>

<p>
Questions, feedback, and bug reports/fixes can be sent to our <a
href="#Mail">mailing lists</a>.
</p>


<h3><a name="Mail">Mailing Lists</a></h3>
<p>
We have 3 mailing lists for the Stanford Coreference Rersolution System, all of which are shared
with other JavaNLP tools (with the exclusion of the parser). Each address is
at <code>@lists.stanford.edu</code>:
</p>
<ol>
<li><code>java-nlp-user</code> This is the best list to post to in order
to ask questions, make announcements, or for discussion among JavaNLP
users.  You have to subscribe to be able to use it.
Join the list via <a href="https://mailman.stanford.edu/mailman/listinfo/java-nlp-user">this webpage</a> or by emailing
<code>java-nlp-user-join@lists.stanford.edu</code>.   (Leave the
subject and message body empty.)  You can also
<a href="https://mailman.stanford.edu/pipermail/java-nlp-user/">look at
the list archives</a>.
<li><code>java-nlp-announce</code> This list will be used only to announce
new versions of Stanford JavaNLP tools.  So it will be very low volume (expect 1-3
messages a year).  Join the list via <a href="https://mailman.stanford.edu/mailman/listinfo/java-nlp-announce">this webpage</a> or by emailing
<code>java-nlp-announce-join@lists.stanford.edu</code>.  (Leave the
subject and message body empty.)
<li><code>java-nlp-support</code> This list goes only to the software
maintainers.  It's a good address for licensing questions, etc.  <b>For
general use and support questions, you're better off joining and using
<code>java-nlp-user</code>.</b>
You cannot join <code>java-nlp-support</code>, but you can mail questions to
<code>java-nlp-support@lists.stanford.edu</code>.
</ol>


<h3><a name="Demo">Online Demo</a></h3>
<br>

We have an <a href="http://nlp.stanford.edu:8080/corenlp/">online demo</a> 
of CoreNLP which uses the default annotators.  You can either see the
xml output or see a nicely formatted xsl version of the output.

<br>

<h3><a name="History">Release History</a></h3>
<BR>
<table>
<tr>
  <td><a href="stanford-corenlp-full-2012-11-12.zip">Version 1.3.4</a></td>
  <td>2012-11-12</td>
  <td>Upgrades to sutime, dependency extraction code and English 3-class NER model
    <a href="stanford-corenlp-caseless-2012-11-09-models.jar">compatible caseless models</a></td></tr>
<tr>
  <td><a href="stanford-corenlp-2012-07-09.tgz">Version 1.3.3</a></td>
  <td>2012-07-09</td>
  <td>Minor bug fixes;
    <a href="stanford-corenlp-caseless-2012-07-04-models.jar">compatible caseless models</a></td></tr>
<tr>
  <td><a href="stanford-corenlp-2012-05-22.tgz">Version 1.3.2</a></td>
  <td>2012-05-22</td>
  <td>Upgrades to sutime, include tokenregex annotator;
      <a href="stanford-corenlp-caseless-2012-05-22-models.jar">compatible caseless models</a></td></tr>
<tr>
  <td><a href="stanford-corenlp-2012-04-09.tgz">Version 1.3.1</a></td>
  <td>2012-04-09</td>
  <td>Fixed thread safety bugs, caseless models available: 
      <a href="stanford-corenlp-caseless-2012-04-09-models.jar">compatible caseless models</a></td></tr>
<tr>
  <td><a href="stanford-corenlp-2012-01-08.tgz">Version 1.3.0</a></td>
  <td>2012-01-08</td>
  <td>Fix a crashing bug, fix excessive warnings, threadsafe</td></tr>
<tr>
  <td><a href="stanford-corenlp-2011-09-14.tgz">Version 1.2.0</a></td>
  <td>2011-09-14</td>
  <td>Added SUTime time phrase recognizer to NER, bug fixes, reduced
  library dependencies</td></tr>
<tr>
  <td><a href="stanford-corenlp-v1.1.0.tgz">Version 1.1.0</a></td>
  <td>2011-06-19</td>
  <td>Greatly improved coref results</td></tr>
<tr>
  <td><a href="stanford-corenlp-v1.0.4.tgz">Version 1.0.4</a></td>
  <td>2011-05-15</td>
  <td>DCoref uses less memory, already tokenized input possible</td></tr>
<tr>
  <td><a href="stanford-corenlp-v1.0.3.tgz">Version 1.0.3</a></td>
  <td>2011-04-17</td>
  <td>Add the ability to specify an arbitrary annotator</td></tr>
<tr>
  <td><a href="stanford-corenlp-v1.0.2.tgz">Version 1.0.2</a></td>
  <td>2010-11-11</td>
  <td>Remove wn.jar for license reasons</td></tr>
<tr>
  <td><a href="stanford-corenlp-v1.0.1.tgz">Version 1.0.1</a></td>
  <td>2010-11-10</td>
  <td>Add the ability to remove XML</td></tr>
<tr>
  <td><a href="stanford-corenlp-v1.0.tar.gz">Version 1.0</a></td>
  <td>2010-11-01</td>
  <td>Initial release</td></tr>
</table>

<!--#include virtual="/footer.html" -->

