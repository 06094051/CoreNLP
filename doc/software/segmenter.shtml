<!--#include virtual="/header.html" -->

<center>
<h2><font color="#a40526">Stanford Word Segmenter</font></h2> 
</center>

<center><p><font color="#a40526">
<a href="#Download">Download</a> |
<a href="#Mail">Mailing Lists</a> |
<a href="#History">Release history</a>
</font>
</center>

<p>
  Tokenization of raw text is a standard pre-processing step for many NLP tasks. For English, tokenization usually involves punctuation splitting and separation of some affixes like possessives. Other languages require more extensive token pre-processing, which is usually called <i>segmentation</i>.
</p>

<p>
  The Stanford Word Segmenter currently supports Arabic and Chinese. The provided segmentation schemes have been found to work well for a variety of applications.
</p>

<p>
The system requires Java 1.6+ to be installed. We recommend at least 1G of memory for documents that contain long sentences. For files with shorter sentences (e.g., 20 tokens), decrease the memory requirement by changing the option <code>java -mx1g</code> in the run scripts.
</p>

<h3>Arabic</h3>
<p>
Arabic is a root-and-template language with abundant bound morphemes. These morphemes include possessives, pronouns, and discourse connectives. Segmenting bound morphemes reduces lexical sparsity and simplifies syntactic analysis.
</p>

<p>
The Arabic segmenter model processes raw text according to the Penn Arabic Treebank 3 (ATB) standard. It is a stand-alone implementation of the segmenter described in:
<blockquote>

  Spence Green and John DeNero. 2012. <a href="http://www.spencegreen.com/research/"><i>A Class-Based Agreement Model for Generating Accurately Inflected Translations</i></a>. In ACL.
</blockquote>
</p>

<h3>Chinese</h3>
<p>
Chinese is standardly written without spaces between words (as are some
other languages).  This software will split Chinese text into a sequence
of words, defined according to some word segmentation standard.
It is a Java implementation of the CRF-based Chinese Word Segmenter
described in: 
<blockquote>

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky and Christopher Manning. 2005. <a href="http://nlp.stanford.edu/pubs/sighan2005.pdf"><i>A Conditional Random Field Word Segmenter</i></a>. In Fourth SIGHAN Workshop on Chinese Language Processing.
</blockquote>
</p>

<p>
Two models with two different segmentation standards are included:
<a href="http://www.cis.upenn.edu/~chinese/segguide.3rd.ch.pdf">
Chinese Penn Treebank standard</a> and 
<a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/pku_spec.pdf">
Peking University standard</a>.
</p>

<p>On May 21, 2008, we released a version that makes use of lexicon
features.  With external lexicon features, the segmenter segments more
consistently and also achieves higher F measure when we train and test
on the bakeoff data. This version is close to the CRF-Lex segmenter described in:
<blockquote>
Pi-Chuan Chang, Michel Galley and Chris Manning. 2008. <a href="http://nlp.stanford.edu/pubs/acl-wmt08-cws.pdf"><i>Optimizing Chinese Word Segmentation for Machine Translation Performance</a></i>. In WMT.
</blockquote>

The older version (2006-05-11) without using external lexicon features
will still be available for download, but we do recommend using the
latest version.

<p>
Another new feature of the latest release is that the segmenter can now output k-best segmentations. <a href="http://nlp.stanford.edu/software/trainSegmenter-20080521.tar.gz">
An example of how to train the segmenter</a> is now also available.
</p>

<h3><a name="Download">Download</a></h3>

<p>The segmenter is available for download,
<b>licensed under the <a href="http://www.gnu.org/licenses/gpl-2.0.html">GNU
General Public License</a></b> (v2 or later). Source is included.  
The package includes components for command-line invocation and a Java API.
The segmenter
code is dual licensed (in a similar manner to MySQL, etc.).  
Open source licensing is under the <i>full</i> GPL,
which allows many free uses.
For distributors of 
<a href="http://www.gnu.org/licenses/gpl-faq.html#GPLInProprietarySystem">proprietary
software</a>, <b>commercial licensing</b> with a 
<a href="http://otlportal.stanford.edu/techfinder/technology/ID=27276">ready-to-sign
agreement</a> is available.
If you don't need a commercial license, but would like to support
maintenance of these tools, we welcome gift funding.
</p>

<p>The download is a zipped file consisting of
    model files, compiled code, and source files.  If you unpack the tar file,
    you should have everything needed.  Simple scripts are included to 
    invoke the segmenter. <!-- Please send any questions or feedback, or 
    extensions and bugfixes to:
<a href="mailto:java-nlp-support@lists.stanford.edu"><code>java-nlp-support@lists.stanford.edu</code></a>. -->
</p>


<p>
<font color="#a40526"><a href="stanford-segmenter-2012-11-11.zip">Download
	  Stanford Word Segmenter version 2012-11-11</font></a>
</p>

<h3><a name="Mail">Mailing Lists</a></h3>
<p>
We have 3 mailing lists for the Stanford Word Segmenter, all of which are shared
with other JavaNLP tools (with the exclusion of the parser). Each address is
at <code>@lists.stanford.edu</code>:
</p>
<ol>
<li><code>java-nlp-user</code> This is the best list to post to in order
to ask questions, make announcements, or for discussion among JavaNLP
users.  You have to subscribe to be able to use it.
Join the list via <a href="https://mailman.stanford.edu/mailman/listinfo/java-nlp-user">this webpage</a> or by emailing
<code>java-nlp-user-join@lists.stanford.edu</code>.   (Leave the
subject and message body empty.)  You can also
<a href="https://mailman.stanford.edu/pipermail/java-nlp-user/">look at
the list archives</a>.
<li><code>java-nlp-announce</code> This list will be used only to announce
new versions of Stanford JavaNLP tools.  So it will be very low volume (expect 1-3
message a year).  Join the list via via <a href="https://mailman.stanford.edu/mailman/listinfo/java-nlp-announce">this webpage</a> or by emailing
<code>java-nlp-announce-join@lists.stanford.edu</code>.  (Leave the
subject and message body empty.)
<li><code>java-nlp-support</code> This list goes only to the software
maintainers.  It's a good address for licensing questions, etc.  <b>For
general use and support questions, please join and use
<code>java-nlp-user</code>.</b>
You cannot join <code>java-nlp-support</code>, but you can mail questions to
<code>java-nlp-support@lists.stanford.edu</code>.
</ol>

<h3><a name="History">Release History</a></h3>
<br>
<table cellspacing="5">
<tr>
<th align="left">Version</th><th align="left">Date</th><th align="left">Description</th>
</tr>
<tr valign="top">
  <td><a href="stanford-segmenter-2012-11-11.zip">1.6.7</a></td>
  <td>2012-11-11</td>
  <td>Bugfixes for both Arabic and Chinese, Chinese segmenter can now load data from a jar file</td>
</tr>
<tr valign="top">
  <td><a href="stanford-segmenter-2012-07-09.tgz">1.6.6</a></td>
  <td>2012-07-09</td>
  <td>Improved Arabic model</td>
</tr>
<tr valign="top">
  <td><a href="stanford-segmenter-2012-05-22.tar.gz">1.6.5</a></td>
  <td>2012-05-22</td>
  <td>Fixed encoding problems, supports stdin for Chinese segmenter</td>
</tr>
<tr valign="top">
  <td><a href="stanford-segmenter-2012-05-07.tar.gz">1.6.4</a></td>
  <td>2012-05-07</td>
  <td>Included Arabic model</td>
</tr>
<tr valign="top">
  <td><a href="stanford-chinese-segmenter-2012-01-08.tar.gz">1.6.3</a></td>
  <td>2012-01-08</td>
  <td>Minor bug fixes</td>
</tr>
<tr valign="top">
  <td><a href="stanford-chinese-segmenter-2011-09-14.tar.gz">1.6.2</a></td>
  <td>2011-09-14</td>
  <td>Improved thread safety</td>
</tr>
<tr valign="top">
  <td><a href="stanford-chinese-segmenter-2011-06-19.tar.gz">1.6.1</a></td>
  <td>2011-06-19</td>
  <td>Fixed empty document bug when training new models</td>
</tr>
  <td><a href="stanford-chinese-segmenter-2011-05-15.tar.gz">1.6</a></td>
  <td>2011-05-15</td>
  <td>Models updated to be slightly more accurate; code correctly released so it now builds; updated for compatibility with other Stanford releases</td>
</tr>
<tr valign="top">
  <td><a href="stanford-chinese-segmenter-2008-05-21.tar.gz">1.5</a></td>
  <td>2008-05-21</td>
  <td>(with external lexicon features; 
      able to output k-best segmentations)</td>
</tr>
<tr valign="top">
  <td><a href="StanfordChineseSegmenter-2006-05-11.tar.gz">1.0</a></td>
  <td>2006-05-11</td>
  <td>Initial release</td>
</tr>
</table>
<BR>




<!--#include virtual="/footer.html" -->

