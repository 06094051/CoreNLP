<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <!-- needed for IE rendering -->
  <!-- <meta http-equiv="X-UA-Compatible" content="IE=edge" /> -->
  
  <title>Stanford Topic Modeling Toolbox</title>
  
  <!-- for use with http://alexgorbatchev.com/wiki/SyntaxHighlighter -->
  <script type="text/javascript" src="js/SyntaxHighlighter/scripts/shCore.js"></script>
  <script type="text/javascript" src="js/SyntaxHighlighter/scripts/shBrushScala.js"></script>
  <link type="text/css" rel="stylesheet" href="js/SyntaxHighlighter/styles/shCore.css"/>
  <link type="text/css" rel="stylesheet" href="js/SyntaxHighlighter/styles/shThemeDefault.css"/>
  <script type="text/javascript">
    SyntaxHighlighter.config.clipboardSwf = 'js/SyntaxHighlighter/scripts/clipboard.swf';
    SyntaxHighlighter.all();
  </script>
  
  <!-- jquery -->
  <script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.3/jquery.min.js"></script>

  <!-- fancybox thumbnails -->
  <link rel="stylesheet" type="text/css" href="js/jquery.fancybox/jquery.fancybox.css" media="screen" />
  <script type="text/javascript" src="js/jquery.fancybox/jquery.easing.1.3.js"></script>
  <script type="text/javascript" src="js/jquery.fancybox/jquery.fancybox-1.2.1.pack.js"></script>

  <!-- custom jquery nav code -->
  <script type="text/javascript">
    $(document).ready(function(){
      $("div.sectionbody").hide();
      
      $("a.doShow").click(function(event){
        var base = event.target.id.toString().substring(0,event.target.id.toString().length-".show".length);
        $("div[id="+base+".content]").slideDown();
        $("a.doHide[id="+base+".hide]").show();
        $(this).hide("slow");
        event.preventDefault();
      });
      
      $("a.doHide").click(function(event){
        var base = event.target.id.toString().substring(0,event.target.id.toString().length-".hide".length);
        $("div[id="+base+".content]").slideUp();
        $("a.doShow[id="+base+".show]").show();
        $(this).hide("slow");
        event.preventDefault();
      });
      
      $("a.doPrint").click(function(event){
        $("*").addClass("print");
        $("div.sectionbody").slideDown();
        $("a.doShow").hide();
        $("a.doHide").hide();
        $(this).hide("slow");
        $("a.doScreen").show();
        event.preventDefault();
      });
      
      $("a.doScreen").click(function(event){
        $("*").removeClass("print");
        $("div.sectionbody").slideUp();
        $("a.doShow").show();
        $("a.doHide").hide();
        $(this).hide("slow");
        $("a.doPrint").show();
        event.preventDefault();
      });
      
      $("a.screenshot").fancybox({
        'overlayShow': true
      });
    });
  </script>
   
  <style type="text/css">
    a.doShow { color: #990000; }
    a.doShow:hover { text-decoration: underline; cursor: pointer; }
    a.doHide { color: #990000; display: none; }
    a.doHide:hover { text-decoration: underline; cursor: pointer; }
    
    a.doPrint { color: #990000; float: right; margin-right: 16px; }
    a.doPrint:hover { text-decoration: underline; cursor: pointer; }
    a.doScreen { color: #990000; float: right; margin-right: 16px; display: none; }
    a.doScreen:hover { text-decoration: underline; cursor: pointer; }
    
    div.section {
      width: 100%;
      padding-bottom: 20px;
      margin-top: 20px;
      overflow: hidden;
      clear: left;
    }
    
    div.section h2 {
      border-bottom: 1px solid #990000;
      padding-bottom: .2em;
    }
    

    body {
      margin: 0px 0px 0px 0px;
      padding: 0px 0px 0px 0px;
    }
    
    body.print {
      background-image: none;
    }
  
    div#stripe {
      clear: none;
      position: fixed;
      top: 0px; left: 0px;
      background-image: url('http://nlp.stanford.edu/img/backstripe.gif');
      background-repeat: repeat-y;
      height: 100%;
      width: 21px;
    }
    
    div#stripe.print {
      display: none;
      width: 0px;
      height: 0px;
      clear: none;
    }
  
    div#navbar {
      border: 1px solid #F5EEC6;
      margin-left: 21px;
      background-repeat: no-repeat;
      background-color: #F5EEC6;
    }
    
    div#navbar p {
      font-family: Palatino, Georgia, Book Antiqua, serif;
      font-size: 21px;
      line-height: 24px;
    }
    
    div#navbar a {
      font-size: 12px;
      font-weight: bold;
    }
    
    div#navbar a:link {
      text-decoration: none;
      color: #990000;
      background: #F5EEC6;
    }
    
    div#navbar a:visited {
      text-decoration: none;
      color: #990000;
      background: #F5EEC6;
    }
    
    div#navbar a:hover {
      text-decoration: underline;
    }
    
    div#navbar a:active {
      text-decoration: none;
    }
    
    div#navbar.print {
      visibility:hidden;
      width: 0px;
      height: 0px;
      clear: none;
    }

    div#page {
      margin-left: 35px;
      max-width: 800px;
      text-align: left;
      line-height: 110%;
    }
    
    div#page.print {
      position: static;
      margin-left: 5px;
      margin-top: 0px;
      max-width: 100%;
      width: auto;
      height: auto;
    }
    
    a.screenshot img {
      border: 1px solid gray;
    }
    
    * {
      font-family: Lucida Sans, Arial, Helvetica, sans-serif;
      font-size: small;
      line-height: 1.2em;
    }
    
    h1,h2,h3 {
      font-size: larger;
    }
    
    a img {
      border: none;
    }
  </style>
</head>

<body>

<div id="navbar">
  <table><tr>
  <td>
    <a href="http://nlp.stanford.edu/"><img
      src="http://nlp.stanford.edu/img/nlp-logo-navbar.jpg"
      alt="Stanford NLP" /></a>
  </td>
  <td>
    <p>The Stanford Natural Language Processing Group</p>
    <a href="http://nlp.stanford.edu/">home</a>
    &middot;
    <a href="http://nlp.stanford.edu/people.shtml">people</a>
    &middot;
    <a href="http://nlp.stanford.edu/teaching.shtml">teaching</a>
    &middot;
    <a href="http://nlp.stanford.edu/research.shtml">research</a>
    &middot;
    <a href="http://nlp.stanford.edu/publications.shtml">publications</a>
    &middot;
    <a href="http://nlp.stanford.edu/software/index.shtml">software</a>
    &middot;
    <a href="http://nlp.stanford.edu/events.shtml">events</a>
    &middot;
    <a href="http://nlp.stanford.edu/local/index.shtml">local</a> 
  </td>
  </tr></table>
</div>
<div id="stripe"></div>


<div id="page">

<h1>Stanford Topic Modeling Toolbox</h1>

<div>
<a class="doPrint">
  <img src="printer.png" alt="Printer Icon [www.famfamfam.com]" width="12px" height="12px" />
  Print-friendly version
</a>
<a class="doScreen">Screen version</a>

Version 0.4.0
</div>

<p>The Stanford Topic Modeling Toolbox (TMT) brings topic modeling tools to social scientists and others who wish to perform analysis on datasets that have a substantial textual component.  The toolbox features that ability to:</p>

<a class="screenshot" rel="group" style="float:right; margin-left: 4em; margin-right: 16px" href="screenshot-0.1.1-0-title.png" title="TMT provides a simple way to access trends in medium size text collections."><img src="screenshot-0.1.1-0-title.thumb.png" alt="" /></a>

<ul>
  <li>Import and manipulate text from cells in Excel and other spreadsheets.</li>
  <li>Train topic models (LDA, Labeled LDA, and PLDA <i>new</i>) to create summaries of the text.</li>
  <li>Select parameters (such as the number of topics) via a data-driven process.</li>
  <li>Generate rich Excel-compatible outputs for tracking word usage across topics, time, and other groupings of data.</li>
</ul>

<p>The Stanford Topic Modeling Toolbox was written at the Stanford NLP group by: <br /> <a href="http://cs.stanford.edu/~dramage/">Daniel Ramage</a> and Evan Rosen, first released in September 2009.</p>

<div class="screenshots">
  <a class="screenshot" rel="group" href="screenshot-data-0.png" title="Prepare your data in Excel.  TMT reads standard CSV and TSV files where the text to analyze lives in spreadsheet columns."><img src="screenshot-data-0.thumb.png" alt="" /></a>
  <a class="screenshot" rel="group" href="screenshot-0.1.1-1-ex0.png" title="Load a script from the file menu and prepare to run.  Script access data in CSV and TSV files."><img src="screenshot-0.1.1-1-ex0.thumb.png" alt="" /></a>
  <a class="screenshot" rel="group" href="screenshot-0.1.1-2-postrun.png" title="After clicking Run, the script executes and its output is written into the script output window."><img src="screenshot-0.1.1-2-postrun.thumb.png" alt="" /></a>
  <a class="screenshot" rel="group" href="screenshot-pivot-5.png" title="Load the script output into Excel to generate pivot chart showing topic trends over time."><img src="screenshot-pivot-5.thumb.png" alt="" /></a>
</div>


<table style="width: 90%; margin-left: auto; margin-right: auto; margin-top: 2em;" cellspacing="10" cellpadding="10">

<tr>
<td width="33%" style="border: 1px solid gray" valign="top">
<h3>Download</h3>
<p><img src="icon-application_go.png" alt="download" />&nbsp;<a href="tmt-0.4.0.jar" >tmt-0.4.0.jar</a></p>
<!-- <p><img src="icon-folder_brick.png" alt="examples" /> examples</p> -->
</td>

<td width="33%" style="border: 1px solid gray" valign="top">
<h3>Programmers</h3>
<p><img src="icon-application_go.png" alt="download source" />&nbsp;Source&nbsp;<a href="tmt-0.4.0-src.zip" >tmt-0.4.0-src.zip</a></p>
<p>See the <a href="api/">API Reference</a>.</p>
</td>

<td width="33%" style="border: 1px solid gray" valign="top">
<h3>Upgrading from 0.2.x?</h3>
<p>The example scripts and data file have changed since the previous release.  Make sure to update your scripts accordingly.  (Don't forget to check the imports!)</p>
</td>

</table>


<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Getting started</h2>

<p>This section contains software installation instructions and the overviews the basic mechanics of running the toolbox.</p>

<a class="doShow" id="section.getting-started.show">More ...</a>

<div class="sectionbody" id="section.getting-started.content">

<h3>Prerequisites</h3>

<ul>
  <li>A text editor for creating TMT processing scripts.<br/>
      <i>TMT scripts are written in <a href="http://www.scala-lang.org/">Scala</a>, but no knowledge of Scala is required to get started.</i></li>
  <li>An installation of <a href="http://java.sun.com/">Java 6SE</a> or greater: <a href="http://java.com/en/download/index.jsp">download</a>. <br />
    <i>Windows, Mac, and Linux are supported.</i></li>
</ul>

<h3>Installation</h3>

<ul>
  <li>Download and save the TMT executable to your computer from the link at the top of this page. (Sorry, but it's large!)</li>
  <li>Double-click the file to open toolbox.  You should see a simple GUI.  Or from the command line, you can run: <code>java -jar tmt-0.4.0.jar</code></li>
</ul>

<h3>Example data files and scripts</h3>

<ul>
  <li>Download the example data file: <a href="examples/pubmed-oa-subset.csv">pubmed-oa-subset.csv</a></li>
  <li>Download the first example script: <a href="examples/example-0-test.scala">example-0-test.scala</a></li>
</ul>

These two downloads should go into the same folder so that the script can find the data.

<blockquote>
  <p><b>example-0-test.scala</b></p>
  
  <pre class="brush: scala;">
// TMT Example 0 - Basic data loading

import scalanlp.io._;

val pubmed = CSVFile("pubmed-oa-subset.csv");

println("Success: " + pubmed + " contains " + pubmed.data.size + " records");
  </pre>
</blockquote>

<p>This is a simple script that just loads the records contained in the sample data file pubmed-oa-subset, an subset of the Open Access database of publications in Pubmed.</p>

<h3> Running the toolbox</h3>

<p>Now run the toolbox as before and select "Open script ..." from the file menu.  Navigate to <code>example-0-test.scala</code> and click open, then run.</p>

<p>Alternatively, you can run the script from the command line:</p>

<pre>
  java -jar tmt-0.4.0.jar example-0-test.scala
</pre>

<p>If all goes well you should see the following lines of output:</p>

<pre>
  Success: CSVFile("pubmed-oa-subset.csv") contains 1550 records
</pre>

<p>You're all set to continue with the tutorial. For the rest of the tutorial, invoke the toolbox in the same way as we do above but with a different script name.</p>

<a class="doHide" id="section.getting-started.hide">[close section]</a>
</div>
</div>


<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Preparing a dataset</h2>

<p>The first step in using the Topic Modeling Toolbox on a data file (CSV or TSV, e.g. as exported by Excel) is to tell the toolbox where to find the text in the file.  This section describes how the toolbox converts a column of text from a file into a sequence of words.</p>

<p>The code for this example is in <a href="examples/example-1-dataset.scala">example-1-dataset.scala</a>.</p>

<a class="doShow" id="section.preparing-a-dataset.show">More ...</a>

<div class="sectionbody" id="section.preparing-a-dataset.content">

<p>The process of extracting and preparing text from a CSV file can be thought of as a pipeline, where a raw CSV file goes through a series of stages that ultimately result in something that can be used to train the topic model.  Here is a sample pipeline for the pubmed-oa-subset.csv data file:</p>

<pre class="brush: scala">
val source = CSVFile("pubmed-oa-subset.csv") ~> IDColumn(1);

val tokenizer = {
  SimpleEnglishTokenizer() ~>            // tokenize on space and punctuation
  CaseFolder() ~>                        // lowercase everything
  WordsAndNumbersOnlyFilter() ~>         // ignore non-words and non-numbers
  MinimumLengthFilter(3)                 // take terms with >=3 characters
}

val text = {
  source ~>                              // read from the source file
  Column(4) ~>                           // select column containing text
  TokenizeWith(tokenizer) ~>             // tokenize with tokenizer above
  TermCounter() ~>                       // collect counts (needed below)
  TermMinimumDocumentCountFilter(4) ~>   // filter terms in <4 docs
  TermDynamicStopListFilter(30) ~>       // filter out 30 most common terms
  DocumentMinimumLengthFilter(5)         // take only docs with >=5 terms
}
</pre>

<p>The input data file (in the <code>source</code> variable) is a pointer to the CSV file you downloaded earlier, which we will pass through a series of <em>stages</em> that each transform, filter, or otherwise interact with the data.  In line one, we instruct TMT to use the value in column 1 as the record ID, a unique identifier for each record in the file.  If your sheet's record IDs are in a different column, change the <code>1</code> value in line one above to the appropriate column.  If your sheet does not have a record ID column, you can leave off the "<code>~> IDColumn(1)</code>" and TMT will use the line number in the file as the record id.</p>

<p>If the first row in your CSV file contains the column names, you can remove that row using the <code>Drop</code> stage:</p>

<pre class="brush: scala">
  val source = CSVFile("your-csv-file.csv") ~> IDColumn(yourIdColumn) ~> Drop(1);
</pre>

<h3>Tokenizing</h3>

<p>The first step is to define a tokenizer that will convert the cells containing text in your dataset to terms that the topic model will analyze.  The tokenizer, defined in lines 3 through 7, is specified as a series of transformations that convert a string to a sequence of strings.</p>

<p>First, we use the <code>SimpleEnglishTokenizer()</code> to remove punctuation from the ends of words and then split up the input text by whitespace characters (tabs, spaces, carriage returns, etc.).  You could alternatively use the <code>WhitespaceTokenizer()</code> if your text fields have already been processed into cleaned tokens.  Or you can specify a custom tokenizer based on a regular expression by using the <code>RegexSplitTokenizer("your-regex-pattern")</code>.</p>

<p>The <code>CaseFolder</code> is then used to lower-case each word so that "The" and "tHE" and "THE" all look like "the". Case folding reduces the number of distinct word types seen by the model by turning all character to lowercase.</p>

<p>Next, words that are entirely punctuation and other non-word non-number characters are removed from the generated lists of tokenized documents by using the <code>WordsAndNumbersOnlyFilter</code>.</p>

<p>Finally, terms that are shorter than 3 characters are removed using the <code>MinimumLengthFilter</code>.</p>

<p>Optionally, tokens can be stemmed using a <code>PorterStemmer()</code> stage in the series of transformations just before the MinimumLengthFilter.  Stemming is a common technique in information retrieval to collapse simple variations such as pluralization into a single common term ("books" and "book" both map to "book").  However, stemming doesn't always add value in a topic modeling context, because stemming sometimes combines terms that would best be considered distinct, and variations of the same word will usually end up in the same topic anyway.</p>

<p>If you want to remove standard english language stop words, you can use a <code>StopWordFilter("en")</code> as the last step the tokenizer.</p>

<h3>Extracting and tokenizing text in a CSV file</h3>

<p>After defining the tokenizer, we can use this tokenizer to extract text from the appropriate column(s) in the CSV file.  If your text data is in a single column (here, the fourth column):</p>

<pre class="brush: scala">
  source ~> Column(4) ~> TokenizeWith(tokenizer)
</pre>

<p>The code above will load the text from column four in the CSV file.</p>

<p>If your text is in more than one column:</p>
<pre class="brush: scala">
  source ~> Columns(3,4) ~> Join(" ") ~> TokenizeWith(tokenizer)
</pre>

<p>The code above select columns two and three, and then concatenates their contents with a space character used as glue.</p>

<h3>Finding meaningful words</h3>

<p>Topic models can be useful for extracting patterns in meaningful word use, but they are not good at determining which words are meaningful. It is often the case that the use of very common words like 'the' do not indicate the type of similarity between documents in which one is interested.  To lead LDA towards extracting patterns among meaningful words, we implemented a collection of standard heuristics:</p>

<pre class="brush: scala">
  ... ~>
  TermCounter ~>
  TermMinimumDocumentCountFilter(4) ~>
  TermDynamicStopListFilter(30) ~>
  ...
</pre>

<p>The code above removes terms appear in less than four documents (because very rare words tell us little about the similarity of documents), and 30 most common words in the corpus (because words that are ubiquitous also tell us little about the similarity of documents, they are removed and conventionally denoted "stop words").  These values might need to be updated if you are working with a much larger or much smaller corpus than a few thousand documents.</p>

<p>If you have an explicit list of stop words you would like removed from your corpus, you can add an extra stage to the processing: <code>TermStopListFilter(List("positively","scrumptious"))</code>.  Here, add as many terms as you like, in quotes, within the <code>List</code>.  Remember that the <code>TermStopListFilter</code> is run <em>after</em> the text has already been tokenized, so the list you provide should match the output of your tokenizer - e.g. terms should already be lower-cased and stemmed if your tokenizer includes a <code>CaseFolder</code> and <code>PorterStemmer</code>.

<p>The <code>TermCounter</code> stage must first compute some statistics needed by the next stages.  These statistics are stored in the metadata associated with each parcel, which enables any downstream stage to access that information.  This stage will also generate a cache file on disk in the same folder as the underlying CSVFile that saves the stored document statistics.  The file name will start with the name of the underlying CSV file and include a signature of the pipeline "term-counts.cache".</p>

<h3>Removing Empty Documents</h3>

Some documents in your dataset may be missing or empty (now that some words were filtered in the last step).  We can disregard these documents during training by applying the <code>DocumentMinimumLengthFilter(length)</code> to remove all documents shorter than the specified length.

<h3>Putting it all together</h3>

<p>Run example 1 (<a href="examples/example-1-dataset.scala">example-1-dataset.scala</a>).  This program will first load the data pipeline and then print out information about the loaded text dataset, including a signature of the dataset and the list of 30 stop words found for this corpus.  <i>[Note that in PubMed, "gene" is filtered out because it is so common!]</i></p>

<a class="doHide" id="section.preparing-a-dataset.hide">[close section]</a>
</div>
</div>


<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Learning a topic model</h2>

<p>Once you've prepared a dataset to learn against, you're all set to train a topic model. This example shows how to train an instance of Latent Dirichlet Allocation using the dataset you prepared above.</p>

<p>The code for this example is in <a href="examples/example-2-lda-learn.scala">example-2-lda-learn.scala</a></p>

<a class="doShow" id="section.learning-lda.show">More ...</a>

<div class="sectionbody" id="section.learning-lda.content">

<h3>Load the data file</h3>

<pre class="brush: scala">
val source = CSVFile("pubmed-oa-subset.csv") ~> IDColumn(1);

val tokenizer = {
  SimpleEnglishTokenizer() ~>            // tokenize on space and punctuation
  CaseFolder() ~>                        // lowercase everything
  WordsAndNumbersOnlyFilter() ~>         // ignore non-words and non-numbers
  MinimumLengthFilter(3)                 // take terms with >=3 characters
}

val text = {
  source ~>                              // read from the source file
  Column(4) ~>                           // select column containing text
  TokenizeWith(tokenizer) ~>             // tokenize with tokenizer above
  TermCounter() ~>                       // collect counts (needed below)
  TermMinimumDocumentCountFilter(4) ~>   // filter terms in <4 docs
  TermDynamicStopListFilter(30) ~>       // filter out 30 most common terms
  DocumentMinimumLengthFilter(5)         // take only docs with >=5 terms
}
</pre>

This code snippet is the same as in the previous tutorial.  It extracts and prepares the text from the example dataset.

<h3>Select parameters for training an LDA model</h3>

<pre class="brush:scala">
// turn the text into a dataset ready to be used with LDA
val dataset = LDADataset(text);

// define the model parameters
val params = LDAModelParams(numTopics = 30, dataset = dataset);
</pre>

Here, you can specify any number of topics you'd like to learn.  You can optionally specify alternative parameters for the Dirichlet term and topic smoothing used by the LDA model by providing extra arguments to the <code>LDAModelParams</code> constructor on line 5: by default, line 5 is equivalent assumes <code>termSmoothing=SymmetricDirichletParams(.1)</code> and <code>topicSmoothing=SymmetricDirichletParams(.1)</code> have been provided as arguments.

<h3>Train the model to fit the documents</h3>

<p>As of version 0.3, the toolkit supports multiple forms of learning and inference on most topic models, including the default form that supports multi-threaded training and inference on modern multi-core machines.  Specifically, the model can use a collapsed Gibbs sampler <span style="font-size:small">[T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. PNAS, 1:5228–35]</span> or the collapsed variational Bayes approximation to the LDA objective <span style="font-size:small">[Asuncion, A., Welling, M., Smyth, P., & Teh, Y. W. (2009). On Smoothing and Inference for Topic Models. UAI 2009]</span>.</p>

<pre class="brush: scala;">
// Name of the output model folder to generate
val modelPath = file("lda-"+dataset.signature+"-"+params.signature);

// Trains the model: the model (and intermediate models) are written to the
// output folder.  If a partially trained model with the same dataset and
// parameters exists in that folder, training will be resumed.
TrainCVB0LDA(params, dataset, output=modelPath, maxIterations=1000);

// To use the Gibbs sampler for inference, instead use
// TrainGibbsLDA(params, dataset, output=modelPath, maxIterations=1500);
</pre>

<p>The model will output status messages as it trains, and will write the generated model into a folder in the current directory named, in this case "lda-59ea15c7-30-75faccf7".  It'll take a few minutes.  Note that, by default, training using CVB0LDA will use as many processing cores as are available on the machine, and, because of its faster convergence rates, CVB0LDA needs to run for fewer iterations than GibbsLDA.  However, GibbsLDA requires less memory during training.</p>

<h3>A tour of the generated output folder</h3>

<p>The generated model output folder, in this case <code>lda-59ea15c7-30-75faccf7</code>, contains everything needed to analyze the learning process and to load the model back in from disk.</p>

<table>
  <tr><td width="250px">description.txt</td>
      <td>A description of the model saved in this folder.</td></tr>
  <tr><td>document-topic-distributions.csv</td>
      <td>A csv file containing the per-document topic distribution for each document in the dataset.</td></tr>
  <tr><td>[Snapshot]: 00000 - 01000</td>
      <td>Snapshots of the model during training.</td></tr>
  <tr><td>[Snapshot]/params.txt</td>
      <td>Model parameters used during training.</td></tr>
  <tr><td>[Snapshot]/tokenizer.txt</td>
      <td>Tokenizer used to tokenize text for use with this model.</td></tr>
  <tr><td>[Snapshot]/summary.txt</td>
      <td>Human readable summary of the topic model, with top-20 terms per topic and how many words instances of each have occurred.</td></tr>
  <tr><td>[Snapshot]/log-probability-estimate.txt</td>
      <td>Estimate of the log probability of the dataset at this iteration.</td></tr>
  <tr><td>[Snapshot]/term-index.txt</td>
      <td>Mapping from terms in the corpus to ID numbers (by line offset).</td></tr>
  <tr><td>[Snapshot]/topic-term-distributions.csv.gz</td>
      <td>For each topic, the probability of each term in that topic.</td></tr>
</table>

<h3>Determining if the model has converged</h3>

<p>A simple way to see if the training procedure on the model has converged is to look at the values in the numbered folders of <code>log-probability-estimate.txt</code>. This file contains an informal estimate of the model's estimation of the probability of the data while training. The numbers tend to make a curve that tapers off but never stops changing completely. If the numbers don't look like they've stabilized, you might want to retrain using a higher number of iterations.</p>

<a class="doHide" id="section.learning-lda.hide">[close section]</a>

</div>
</div>



<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Topic model inference on a new corpus</h2>

<p>During training, the toolbox records the per-document topic distribution for each training document in a file called document-topic-distributions.csv in the generated model folder.  After a model is trained, it can be used to analyze another (possibly larger) body of text, a process called inference.  This tutorial shows how to perform inference on a new dataset using an existing topic model.</p>

<p>The code for this example is in <a href="examples/example-3-lda-infer.scala">example-3-lda-infer.scala</a></p>

<a class="doShow" id="section.tmt-infer.show">More ...</a>

<div class="sectionbody" id="section.tmt-infer.content">

<h3>Load the trained LDA model</h3>

<pre class="brush: scala">
// the path of the model to load
val modelPath = file("lda-59ea15c7-30-75faccf7");

println("Loading "+modelPath);
val model = LoadCVB0LDA(modelPath);
// Or, for a Gibbs model, use:
// val model = LoadGibbsLDA(modelPath);
</pre>

<p>Here, we re-load the model trained in the previous example.</p>

<h3>Load the new dataset for inference</h3>
<pre class="brush: scala">
// A new dataset for inference.  (Here we use the same dataset
// that we trained against, but this file could be something new.)
val source = CSVFile("pubmed-oa-subset.csv") ~> IDColumn(1);

val text = {
  source ~>                              // read from the source file
  Column(4) ~>                           // select column containing text
  TokenizeWith(model.tokenizer.get)      // tokenize with existing model's tokenizer
}

// Base name of output files to generate
val output = file(modelPath, source.meta[java.io.File].getName.replaceAll(".csv",""));

// turn the text into a dataset ready to be used with LDA
val dataset = LDADataset(text, termIndex = model.termIndex);
</pre>

<p>Here, we prepare the new dataset, tokenizing with the loaded model's original tokenizer.  Note: in this particular example, we are actually using the same file as we trained against.  In expected use, the referenced <code>CSVFile</code> would be to a different file on disk.</p>

<p>We also construct the base of the output path file name - the generated files, below, will go into the model folder with a name that starts with the name of the inference dataset.</p>

<h3>Infer per-document distributions over latent topics</h3>

<pre class="brush: scala;">
println("Writing document distributions to "+output+"-document-topic-distributions.csv");
val perDocTopicDistributions = InferCVB0DocumentTopicDistributions(model, dataset);
CSVFile(output+"-document-topic-distributuions.csv").write(perDocTopicDistributions);

println("Writing topic usage to "+output+"-usage.csv");
val usage = QueryTopicUsage(model, dataset, perDocTopicDistributions);
CSVFile(output+"-usage.csv").write(usage);
</pre>

<p>Here, we infer the per-document topic distributions for each document in the inference dataset, writing these distributions to a new CSV file in the model folder.  We also write a file containing how often each topic is used in the inference dataset.</p>

<h3>Infer per-word distributions over latent topics</h3>

<pre class="brush: scala;">
println("Estimating per-doc per-word topic distributions");
val perDocWordTopicDistributions = EstimatePerWordTopicDistributions(
  model, dataset, perDocTopicDistributions);

println("Writing top terms to "+output+"-top-terms.csv");
val topTerms = QueryTopTerms(model, dataset, perDocWordTopicDistributions, numTopTerms=50);
CSVFile(output+"-top-terms.csv").write(topTerms);
</pre>

<p>Because it differs from the dataset on which the model was trained, we expect that the inference dataset may make use of each learned topic in a way that is not exactly the same as how the topics were used during training.  The toolbox can generate the top-k terms per topic as seen through the lens of the inference dataset.  Here, we generate these top-k terms into the "-top-terms.csv" file.  This file should be compared to the output in summary.txt or the output from running inference on the training dataset as a sanity check to ensure that the topics are used in a qualitatively similar way in the inference dataset as in the training dataset.</p>

<a class="doHide" id="section.tmt-infer.hide">[close section]</a>

</div>
</div>


<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Slicing a topic model's output</h2>

<p>Documents are often associated with explicit metadata, such as the year a document was published, its source, authors, etc.  In this tutorial, we show how TMT can be used to examine how a topic is used in each <em>slice</em> of the data, where a slice is the subset associated with one or more categorical variables.</p>

<p>The code for this example is in <a href="examples/example-4-lda-slice.scala">example-4-lda-slice.scala</a></p>

<a class="doShow" id="section.tmt-slice.show">More ...</a>

<div class="sectionbody" id="section.tmt-slice.content">

<h3>Load the trained LDA model</h3>
<pre class="brush: scala">
// the path of the model to load
val modelPath = file("lda-59ea15c7-30-75faccf7");

println("Loading "+modelPath);
val model = LoadCVB0LDA(modelPath);
// Or, for a Gibbs model, use:
// val model = LoadGibbsLDA(modelPath);
</pre>

<p>As before, we re-load a model from disk.</p>

<h3>Prepare the dataset</h3>

<pre class="brush: scala;">
// A dataset for inference; here we use the training dataset
val source = CSVFile("pubmed-oa-subset.csv") ~> IDColumn(1);

val text = {
  source ~>                              // read from the source file
  Column(4) ~>                           // select column containing text
  TokenizeWith(model.tokenizer.get)      // tokenize with existing model's tokenizer
}

// turn the text into a dataset ready to be used with LDA
val dataset = LDADataset(text, termIndex = model.termIndex);
</pre>

<p>Here, we re-load the same dataset used for training, but a different dataset could be used instead.</p>

<h3>Select column variables for slicing</h3>

<pre class="brush: scala">
// define fields from the dataset we are going to slice against
val slice = source ~> Column(2);
// could be multiple columns with: source ~> Columns(2,7,8)
</pre>

In our example, the year each document was written is stored in column 2, which we will use as our categorical variable for slicing the dataset.  If you want to slice by multiple categorical variables, you can use the <code>Columns</code> stage.

<h3>Load or infer per-document topic distributions</h3>

<pre class="brush: scala">
println("Loading document distributions");
val perDocTopicDistributions = LoadLDADocumentTopicDistributions(
  CSVFile(modelPath,"document-topic-distributions.csv"));
// This could be InferCVB0DocumentTopicDistributions(model, dataset)
// for a new inference dataset.  Here we load the training output.
</pre>

<p>The code above loads the per-document topic distributions generated during training; if you use a different dataset for inference, you may need to replace <code>LoadLDADocumentTopicDistributions</code> with <code>InferCVB0DocumentTopicDistributions</code> or load from a different path.</p>

<h3>Slicing the LDA query output</h3>

<pre class="brush:scala;">
println("Writing topic usage to "+output+"-sliced-usage.csv");
val usage = QueryTopicUsage(model, dataset, perDocTopicDistributions, grouping=slice);
CSVFile(output+"-sliced-usage.csv").write(usage);

println("Estimating per-doc per-word topic distributions");
val perDocWordTopicDistributions = EstimatePerWordTopicDistributions(
  model, dataset, perDocTopicDistributions);

println("Writing top terms to "+output+"-sliced-top-terms.csv");
val topTerms = QueryTopTerms(model, dataset, perDocWordTopicDistributions, numTopTerms=50, grouping=slice);
CSVFile(output+"-sliced-top-terms.csv").write(usage);
</pre>

<p>Here, we generate the usage of each topic in the dataset <em>by slice</em> of the data.  Like the <ocde>QueryTopicUsage</code> line in the previous example, we ask the model how many (fractional) documents and words are associated with each topic.  Here, however, the generated CSV file will contain an extra column breaking down topic usage by group.  In a later tutorial, we'll analyze this output in Excel to plot the usage of each topic over time.</p>

<p>We also generate the top words associated with each topic within each group.  The generated -sliced-top-terms.csv file can be used to analyze if topics are used consistently across sub-groups.</p>

<a class="doHide" id="section.tmt-slice.hide">[close section]</a>

</div>
</div>


<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Analyzing topic model outputs in Excel</h2>

<p>The CSV files generated in the previous tutorial can be directly imported into Excel to provide an advanced analysis and plotting platform for understanding, plotting, and manipulating the topic model outputs. If things don't seem to make sense, you might need to try different model parameters.</p>

<p>The screenshots below were based on the output generated in version 0.1.2.  As of 0.3, each generated output file contains a "Documents" column and a "Words" column.  The first contains the total number of documents associated with each topic within each slice, and the second contains the total number of words associated with each topic within each slice.  Note that both of these numbers will be decimals because LDA assigns each word in the corpus not to a single topic, but to a distribution over topics.</p>

<div class="screenshots">
  <a class="screenshot" rel="pivot" href="screenshot-pivot-0.png" title="Load the sliced LDA output into excel, add a header row, and insert a pivot chart."><img src="screenshot-pivot-0.thumb.png" alt="" /></a>
  <a class="screenshot" rel="pivot" href="screenshot-pivot-1.png" title="If you already selected all cells, just press OK."><img src="screenshot-pivot-1.thumb.png" alt="" /></a>
  <a class="screenshot" rel="pivot" href="screenshot-pivot-2.png" title="Now add Year to Axis fields, Topic to Legend fields and Counts to Value fields."><img src="screenshot-pivot-2.thumb.png" alt="" /></a>
  <a class="screenshot" rel="pivot" href="screenshot-pivot-3.png" title="Change the chart type to a Line graph ..."><img src="screenshot-pivot-3.thumb.png" alt="" /></a>
  <a class="screenshot" rel="pivot" href="screenshot-pivot-4.png" title="And select which topics to show by clicking the drop-down filter by Column Labels."><img src="screenshot-pivot-4.thumb.png" alt="" /></a>
  <a class="screenshot" rel="pivot" href="screenshot-pivot-5.png" title="Now you can explore topic trends over time."><img src="screenshot-pivot-5.thumb.png" alt="" /></a>
</div>

<!--
<a class="doShow" id="section.excel-analysis.show">More ...</a>

<div class="sectionbody" id="section.excel-analysis.content">

</div>
-->

</div>


<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Selecting model parameters</h2>

<p>This tutorial describes how to select model parameters such as the number of topics by a (computationally intensive) tuning procedure, which searches for the parameters that minimize the model's perplexity on held-out data.</p>

<p>The code for this example is in <a href="examples/example-5-lda-select.scala">example-5-lda-select.scala</a></p>

<a class="doShow" id="section.tmt-tuning.show">More ...</a>

<div class="sectionbody" id="section.tmt-tuning.content">

<p>The script splits a document into two subsets: one used for training models, the other used for evaluating their <em>perplexity</em> on unseen data.  Perplexity is scored on the evaluation documents by first splitting each document in half.  The per-document topic distribution is estimated on the first half of the words.  The toolbox then computes an average of how surprised it was by the words in the second half of the document, where surprise is measured in the number of equiprobable word choices, on average.  The value is written to the console, with lower numbers meaning a surer model.</p>

<p>The perplexity scores are not comparable across corpora because they will be affected by different vocabulary size.  However, they can be used to compare models trained on the same data (as in the example script).  However, be aware that models with better perplexity scores don't always produce more interpretable topics or topics better suited to a particular task.  Perplexity scores can be used as stable measures for picking among alternatives, for lack of a better option.  In general, we expect the perplexity to go down as the number of topics increases, but that the successive decreases in perplexity will get smaller and smaller.  A good rule of thumb is to pick a number of topics that produces reasonable output (by inspection of summary.txt) and after the perplexity has started to decrease at a </p>

<p>Some non-parametric topic models can automatically select the number of topics as part of the model training procedure itself.  However, these models (such as the Hierarchical Dirichlet Process) are not yet implemented in the toolbox.  Even in such models, some parameters remain to be tuned, such as the topic smoothing and term smoothing parameters.</p>

<a class="doHide" id="section.tmt-tuning.hide">[close section]</a>

</div>
</div>


<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Training a Labeled LDA model</h2>

<p>Labeled LDA is a supervised topic model for credit attribution in multi-labeled corpora [<a href="http://www.aclweb.org/anthology-new/D/D09/D09-1026.pdf">pdf</a>, <a href="http://www-nlp.stanford.edu/pubs/llda-emnlp09.bib">bib</a>].  If one of the columns in your input text file contains labels or tags that apply to the document, you can use Labeled LDA to discover which parts of each document go with each label, and to learn  accurate models of the words best associated with each label globally.</p>

<p>The code for this example is in <a href="examples/example-6-llda-learn.scala">example-6-llda-learn.scala</a></p>

<a class="doShow" id="section.llda.show">More ...</a>

<div class="sectionbody" id="section.llda.content">

<p>This example is very similar to the example on training a regular LDA model, except for a few small changes.  First, to specify a LabeledLDA dataset, we need to tell the toolbox where the text comes from as well as where the labels come from.  In general, Labeled LDA is useful only when each document has more than one label (otherwise, the model is equivalent to Naive Bayes); but in this example, we will use the year column as our label to model.  Note that because each document has only a single year, the model will actually converge after a single iteration of training, but the example is structured to work for documents that have multiple labels or tags, space separted, in a single column of the source file.</p>

<pre class="brush: scala;">
val source = CSVFile("pubmed-oa-subset.csv") ~> IDColumn(1);

val tokenizer = {
  SimpleEnglishTokenizer() ~>            // tokenize on space and punctuation
  CaseFolder() ~>                        // lowercase everything
  WordsAndNumbersOnlyFilter() ~>         // ignore non-words and non-numbers
  MinimumLengthFilter(3)                 // take terms with >=3 characters
}

val text = {
  source ~>                              // read from the source file
  Column(4) ~>                           // select column containing text
  TokenizeWith(tokenizer) ~>             // tokenize with tokenizer above
  TermCounter() ~>                       // collect counts (needed below)
  TermMinimumDocumentCountFilter(4) ~>   // filter terms in <4 docs
  TermDynamicStopListFilter(30) ~>       // filter out 30 most common terms
  DocumentMinimumLengthFilter(5)         // take only docs with >=5 terms
}

// define fields from the dataset we are going to slice against
val labels = {
  source ~>                              // read from the source file
  Column(2) ~>                           // take column two, the year
  TokenizeWith(WhitespaceTokenizer()) ~> // turns label field into an array
  TermCounter() ~>                       // collect label counts
  TermMinimumDocumentCountFilter(10)     // filter labels in < 10 docs
}

val dataset = LabeledLDADataset(text, labels);
</pre>

<p>Labeled LDA assumes that each document can use only topics that are named in the label set.  Here each document participates in only one label (its year).    Years are not particularly interesting labels (versus, say, a field that contained multiple tags describing each paper), but it suffices for this example.</p>

<p>Training a <code>GibbsLabeledLDA</code> or <code>CVB0LabeledLDA</code> model is similar to training an LDA model.</p>

<pre class="brush: scala;">
// define the model parameters
val modelParams = LabeledLDAModelParams(dataset);

// Name of the output model folder to generate
val modelPath = file("llda-cvb0-"+dataset.signature+"-"+modelParams.signature);

// Trains the model, writing to the givne output path
TrainCVB0LabeledLDA(modelParams, dataset, output = modelPath, maxIterations = 1000);
// or could use TrainGibbsLabeledLDA(modelParams, dataset, output = modelPath, maxIterations = 1500);
</pre>

<p>The LabeledLDA model can be used analogously to an LDA model by adapting the previous examples to the labeled setting, as appropriate.</p>

<a class="doHide" id="section.llda.hide">[close section]</a>

</div>
</div>


<!-- ++++++++++++++++++++++++ -->

<div class="section">
<h2>Training a PLDA model</h2>

<p>Partially Labeled Dirchlet Allocation (PLDA) [<a href="http://dl.acm.org/citation.cfm?id=2020481">paper</a>] is a topic model that extends and generalizes both LDA and Labeled LDA.  The model is analogous to Labeled LDA except that it allows more than one latent topic per label and a set of background labels.  Learning and inference in the model is much like the example above for Labeled LDA, but you must additionally specify the number of topics associated with each label.</p>

<p>Code for this example is in <a href="examples/example-7-plda-learn.scala">example-7-plda-learn.scala</a></p>

</div>
</div>


</div>
</body>
</html>

